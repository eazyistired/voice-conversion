{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34ff30d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-24 18:31:41--  https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
      "Resolving data.keithito.com (data.keithito.com)... 169.150.247.33, 2400:52e0:1e00::1077:1\n",
      "Connecting to data.keithito.com (data.keithito.com)|169.150.247.33|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2748572632 (2,6G) [text/plain]\n",
      "Saving to: â€˜LJSpeech-1.1.tar.bz2â€™\n",
      "\n",
      "LJSpeech-1.1.tar.bz 100%[===================>]   2,56G  3,10MB/s    in 15m 5s  \n",
      "\n",
      "2025-05-24 18:46:47 (2,90 MB/s) - â€˜LJSpeech-1.1.tar.bz2â€™ saved [2748572632/2748572632]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
    "!tar -xf LJSpeech-1.1.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b468b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIFIGAN\n",
    "import os\n",
    "\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.vocoder.configs import HifiganConfig\n",
    "from TTS.vocoder.datasets.preprocess import load_wav_data\n",
    "from TTS.vocoder.models.gan import GAN\n",
    "\n",
    "output_path = 'fine_tune_hifihan_ljspeech'\n",
    "\n",
    "config = HifiganConfig(\n",
    "    batch_size=32,\n",
    "    eval_batch_size=16,\n",
    "    num_loader_workers=8,\n",
    "    num_eval_loader_workers=4,\n",
    "    run_eval=False,\n",
    "    test_delay_epochs=1,\n",
    "    epochs=10,\n",
    "    seq_len=4096,\n",
    "    pad_short=2000,\n",
    "    use_noise_augment=False,\n",
    "    eval_split_size=5,\n",
    "    print_step=25,\n",
    "    print_eval=False,\n",
    "    mixed_precision=True,\n",
    "    lr_gen=1e-4,\n",
    "    lr_disc=1e-4,\n",
    "    data_path=\"LJSpeech-1.1/wavs/\",\n",
    "    output_path=output_path,\n",
    ")\n",
    "\n",
    "# init audio processor\n",
    "ap = AudioProcessor(**config.audio.to_dict())\n",
    "\n",
    "# load training samples\n",
    "eval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n",
    "\n",
    "# init model\n",
    "model = GAN(config, ap)\n",
    "\n",
    "# init the trainer and ğŸš€\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(), config, output_path, model=model, train_samples=train_samples[:4_000], eval_samples=eval_samples\n",
    ")\n",
    "trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5e144f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:None\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:45\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /mnt)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      " > Training Environment:\n",
      " | > Backend: Torch\n",
      " | > Mixed precision: True\n",
      " | > Precision: fp16\n",
      " | > Current device: 0\n",
      " | > Num. of GPUs: 1\n",
      " | > Num. of CPUs: 24\n",
      " | > Num. of Torch Threads: 12\n",
      " | > Torch seed: 54321\n",
      " | > Torch CUDNN: True\n",
      " | > Torch CUDNN deterministic: False\n",
      " | > Torch CUDNN benchmark: False\n",
      " | > Torch TF32 MatMul: False\n",
      " > Start Tensorboard: tensorboard --logdir=results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000\n",
      "/mnt/Storage1/grozavu/conversion/.venv/lib/python3.11/site-packages/trainer/trainer.py:552: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "\n",
      " > Model has 15810401 parameters\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/10\u001b[0m\n",
      " --> results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 18:58:23) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 18:58:26 -- STEP: 0/204 -- GLOBAL_STEP: 0\u001b[0m\n",
      "     | > loss: 1.536017656326294  (1.536017656326294)\n",
      "     | > amp_scaler: 32768.0  (32768.0)\n",
      "     | > grad_norm: 0  (0)\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 1.6148  (1.61484956741333)\n",
      "     | > loader_time: 1.2071  (1.2071425914764404)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 18:58:33 -- STEP: 20/204 -- GLOBAL_STEP: 20\u001b[0m\n",
      "     | > loss: 0.848146378993988  (2.670801654458046)\n",
      "     | > amp_scaler: 8192.0  (8601.6)\n",
      "     | > grad_norm: tensor(30.1562, device='cuda:0')  (tensor(41.4731, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3544  (0.35662404298782346)\n",
      "     | > loader_time: 0.0018  (0.0016029238700866698)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 18:58:41 -- STEP: 40/204 -- GLOBAL_STEP: 40\u001b[0m\n",
      "     | > loss: 0.34078019857406616  (1.7069996245205403)\n",
      "     | > amp_scaler: 8192.0  (8396.8)\n",
      "     | > grad_norm: tensor(16.5012, device='cuda:0')  (tensor(33.8941, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3545  (0.35556764602661134)\n",
      "     | > loader_time: 0.0016  (0.0016758441925048828)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 18:58:48 -- STEP: 60/204 -- GLOBAL_STEP: 60\u001b[0m\n",
      "     | > loss: 0.46199896931648254  (1.2976510966817538)\n",
      "     | > amp_scaler: 8192.0  (8328.533333333335)\n",
      "     | > grad_norm: tensor(36.8151, device='cuda:0')  (tensor(34.2493, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3546  (0.35523942708969114)\n",
      "     | > loader_time: 0.0018  (0.0016760746637980142)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 18:58:55 -- STEP: 80/204 -- GLOBAL_STEP: 80\u001b[0m\n",
      "     | > loss: 0.32632237672805786  (1.0719112031161782)\n",
      "     | > amp_scaler: 8192.0  (8294.399999999998)\n",
      "     | > grad_norm: tensor(31.0024, device='cuda:0')  (tensor(33.6439, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3546  (0.35509494543075554)\n",
      "     | > loader_time: 0.0017  (0.0016770541667938232)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 18:59:02 -- STEP: 100/204 -- GLOBAL_STEP: 100\u001b[0m\n",
      "     | > loss: 0.28338515758514404  (0.9295878976583479)\n",
      "     | > amp_scaler: 8192.0  (8273.919999999995)\n",
      "     | > grad_norm: tensor(20.7147, device='cuda:0')  (tensor(32.5742, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3552  (0.35503826379776)\n",
      "     | > loader_time: 0.0017  (0.0016757774353027344)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 18:59:09 -- STEP: 120/204 -- GLOBAL_STEP: 120\u001b[0m\n",
      "     | > loss: 0.29347896575927734  (0.8304042408863702)\n",
      "     | > amp_scaler: 8192.0  (8260.266666666661)\n",
      "     | > grad_norm: tensor(23.8878, device='cuda:0')  (tensor(31.4713, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3546  (0.3550025443236033)\n",
      "     | > loader_time: 0.0017  (0.0016774654388427735)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 18:59:16 -- STEP: 140/204 -- GLOBAL_STEP: 140\u001b[0m\n",
      "     | > loss: 0.2694060802459717  (0.7551388323307037)\n",
      "     | > amp_scaler: 8192.0  (8250.51428571428)\n",
      "     | > grad_norm: tensor(19.6899, device='cuda:0')  (tensor(30.2505, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.355  (0.35498570374080113)\n",
      "     | > loader_time: 0.0017  (0.0016826033592224122)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 18:59:23 -- STEP: 160/204 -- GLOBAL_STEP: 160\u001b[0m\n",
      "     | > loss: 0.2618262469768524  (0.6967030011117455)\n",
      "     | > amp_scaler: 8192.0  (8243.19999999999)\n",
      "     | > grad_norm: tensor(17.1750, device='cuda:0')  (tensor(29.2290, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3547  (0.354979583621025)\n",
      "     | > loader_time: 0.0017  (0.001682966947555542)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 18:59:30 -- STEP: 180/204 -- GLOBAL_STEP: 180\u001b[0m\n",
      "     | > loss: 0.3170132637023926  (0.6498836015661559)\n",
      "     | > amp_scaler: 8192.0  (8237.51111111111)\n",
      "     | > grad_norm: tensor(23.3402, device='cuda:0')  (tensor(28.1529, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3554  (0.3549796409077114)\n",
      "     | > loader_time: 0.0017  (0.00168228546778361)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 18:59:38 -- STEP: 200/204 -- GLOBAL_STEP: 200\u001b[0m\n",
      "     | > loss: 0.17691875994205475  (0.6096183126419781)\n",
      "     | > amp_scaler: 8192.0  (8232.96)\n",
      "     | > grad_norm: tensor(15.7370, device='cuda:0')  (tensor(27.2215, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.355  (0.35497661948204035)\n",
      "     | > loader_time: 0.0015  (0.0016756534576416015)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 0.2197507917881012  (0.2197507917881012)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 0.2286951094865799  (0.2286951094865799)\n",
      "\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time: 0.0010027885437011719 \u001b[0m(+0)\n",
      "     | > avg_loss: 0.2286951094865799 \u001b[0m(+0)\n",
      "\n",
      " > BEST MODEL : results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000/best_model_204.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 1/10\u001b[0m\n",
      " --> results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 18:59:40) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 18:59:46 -- STEP: 16/204 -- GLOBAL_STEP: 220\u001b[0m\n",
      "     | > loss: 0.24179920554161072  (0.27897077798843384)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(15.7498, device='cuda:0')  (tensor(16.9579, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3551  (0.3551710397005081)\n",
      "     | > loader_time: 0.0018  (0.002108484506607056)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 18:59:54 -- STEP: 36/204 -- GLOBAL_STEP: 240\u001b[0m\n",
      "     | > loss: 0.1858121007680893  (0.25094271492626935)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(13.0158, device='cuda:0')  (tensor(16.4045, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3548  (0.3550041913986206)\n",
      "     | > loader_time: 0.0017  (0.0018986132409837518)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:00:01 -- STEP: 56/204 -- GLOBAL_STEP: 260\u001b[0m\n",
      "     | > loss: 0.16889232397079468  (0.2332105503550598)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(13.9799, device='cuda:0')  (tensor(16.2574, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3547  (0.3549331724643707)\n",
      "     | > loader_time: 0.0017  (0.001823731831141881)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:00:08 -- STEP: 76/204 -- GLOBAL_STEP: 280\u001b[0m\n",
      "     | > loss: 0.1826489269733429  (0.22542668643750643)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(12.1837, device='cuda:0')  (tensor(15.9690, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3548  (0.3549083722265143)\n",
      "     | > loader_time: 0.0017  (0.0017995897092317283)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:00:15 -- STEP: 96/204 -- GLOBAL_STEP: 300\u001b[0m\n",
      "     | > loss: 0.13084958493709564  (0.21889509183044234)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(9.2618, device='cuda:0')  (tensor(15.4610, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3548  (0.3549039935072264)\n",
      "     | > loader_time: 0.0018  (0.0017825365066528322)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:00:22 -- STEP: 116/204 -- GLOBAL_STEP: 320\u001b[0m\n",
      "     | > loss: 0.15142661333084106  (0.21249376622767285)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(13.1878, device='cuda:0')  (tensor(15.2418, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.355  (0.35490262919458854)\n",
      "     | > loader_time: 0.0017  (0.0017732176287420867)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:00:29 -- STEP: 136/204 -- GLOBAL_STEP: 340\u001b[0m\n",
      "     | > loss: 0.1143399104475975  (0.20430278432938984)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(7.4547, device='cuda:0')  (tensor(15.0274, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3548  (0.35490225693758803)\n",
      "     | > loader_time: 0.0017  (0.0017632964779348936)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:00:36 -- STEP: 156/204 -- GLOBAL_STEP: 360\u001b[0m\n",
      "     | > loss: 0.18287265300750732  (0.19924993435732835)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(12.0929, device='cuda:0')  (tensor(14.7202, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3549  (0.3549162164712563)\n",
      "     | > loader_time: 0.0018  (0.0017630503727839547)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:00:44 -- STEP: 176/204 -- GLOBAL_STEP: 380\u001b[0m\n",
      "     | > loss: 0.1562137007713318  (0.1945788310417398)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(11.1179, device='cuda:0')  (tensor(14.5410, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3549  (0.35491354628042726)\n",
      "     | > loader_time: 0.0017  (0.0017580159685828473)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:00:51 -- STEP: 196/204 -- GLOBAL_STEP: 400\u001b[0m\n",
      "     | > loss: 0.16931506991386414  (0.1905979813285629)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(9.5148, device='cuda:0')  (tensor(14.3980, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.355  (0.35492690971919455)\n",
      "     | > loader_time: 0.0016  (0.0017577993626497232)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 0.1368788778781891  (0.1368788778781891)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 0.14780722558498383  (0.14780722558498383)\n",
      "\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.0009751319885253906 \u001b[0m(-2.765655517578125e-05)\n",
      "     | > avg_loss:\u001b[92m 0.14780722558498383 \u001b[0m(-0.08088788390159607)\n",
      "\n",
      " > BEST MODEL : results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000/best_model_408.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 2/10\u001b[0m\n",
      " --> results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 19:00:54) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:00:59 -- STEP: 12/204 -- GLOBAL_STEP: 420\u001b[0m\n",
      "     | > loss: 0.12241078913211823  (0.13851039546231428)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(6.6987, device='cuda:0')  (tensor(12.9157, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3552  (0.3555136521657308)\n",
      "     | > loader_time: 0.0019  (0.0019731918970743814)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:01:06 -- STEP: 32/204 -- GLOBAL_STEP: 440\u001b[0m\n",
      "     | > loss: 0.13044019043445587  (0.13901625876314938)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(12.1736, device='cuda:0')  (tensor(12.6180, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3553  (0.35531318187713623)\n",
      "     | > loader_time: 0.0019  (0.001837514340877533)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:01:14 -- STEP: 52/204 -- GLOBAL_STEP: 460\u001b[0m\n",
      "     | > loss: 0.15087494254112244  (0.1415387695798507)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(12.8305, device='cuda:0')  (tensor(12.0866, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3557  (0.35524879510586077)\n",
      "     | > loader_time: 0.0021  (0.0018490140254680926)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:01:21 -- STEP: 72/204 -- GLOBAL_STEP: 480\u001b[0m\n",
      "     | > loss: 0.13149279356002808  (0.14515615130464235)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(11.6506, device='cuda:0')  (tensor(11.4563, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3555  (0.3552287187841203)\n",
      "     | > loader_time: 0.0019  (0.0018507705794440375)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:01:28 -- STEP: 92/204 -- GLOBAL_STEP: 500\u001b[0m\n",
      "     | > loss: 0.10054102540016174  (0.14309451874831441)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(8.2514, device='cuda:0')  (tensor(11.1937, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3558  (0.3552167104638142)\n",
      "     | > loader_time: 0.0019  (0.0018529218176136847)\n",
      "\n",
      "\n",
      " > CHECKPOINT : results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000/checkpoint_500.pth\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:01:35 -- STEP: 112/204 -- GLOBAL_STEP: 520\u001b[0m\n",
      "     | > loss: 0.10505284368991852  (0.1406278997393591)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(9.3109, device='cuda:0')  (tensor(11.1264, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3554  (0.35521465114184797)\n",
      "     | > loader_time: 0.0018  (0.001853153109550476)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:01:42 -- STEP: 132/204 -- GLOBAL_STEP: 540\u001b[0m\n",
      "     | > loss: 0.1204138845205307  (0.13759501549330624)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(11.3093, device='cuda:0')  (tensor(11.1009, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3557  (0.3552094282525959)\n",
      "     | > loader_time: 0.002  (0.0018535960804332387)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:01:49 -- STEP: 152/204 -- GLOBAL_STEP: 560\u001b[0m\n",
      "     | > loss: 0.11345554888248444  (0.13503598622781662)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(3.5418, device='cuda:0')  (tensor(10.9286, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3553  (0.3552086761123256)\n",
      "     | > loader_time: 0.002  (0.0018521390463176527)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:01:57 -- STEP: 172/204 -- GLOBAL_STEP: 580\u001b[0m\n",
      "     | > loss: 0.14796918630599976  (0.13661134286328802)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(11.4035, device='cuda:0')  (tensor(10.5093, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.356  (0.35522618404654566)\n",
      "     | > loader_time: 0.0021  (0.001859010652054188)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:02:04 -- STEP: 192/204 -- GLOBAL_STEP: 600\u001b[0m\n",
      "     | > loss: 0.12782758474349976  (0.136955564841628)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(4.3581, device='cuda:0')  (tensor(10.2225, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3551  (0.35522842779755587)\n",
      "     | > loader_time: 0.0017  (0.001859072595834732)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 0.1576773077249527  (0.1576773077249527)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 0.13145320117473602  (0.13145320117473602)\n",
      "\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.0010159015655517578 \u001b[0m(+4.076957702636719e-05)\n",
      "     | > avg_loss:\u001b[92m 0.13145320117473602 \u001b[0m(-0.016354024410247803)\n",
      "\n",
      " > BEST MODEL : results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000/best_model_612.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 3/10\u001b[0m\n",
      " --> results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 19:02:08) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:02:12 -- STEP: 8/204 -- GLOBAL_STEP: 620\u001b[0m\n",
      "     | > loss: 0.1784479320049286  (0.12585000414401293)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(4.0146, device='cuda:0')  (tensor(5.4766, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3552  (0.3552277088165283)\n",
      "     | > loader_time: 0.0025  (0.002489030361175537)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:02:19 -- STEP: 28/204 -- GLOBAL_STEP: 640\u001b[0m\n",
      "     | > loss: 0.12172926217317581  (0.13583622260817457)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(9.9086, device='cuda:0')  (tensor(7.1811, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3549  (0.3552196707044329)\n",
      "     | > loader_time: 0.0018  (0.0019459809575762073)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:02:27 -- STEP: 48/204 -- GLOBAL_STEP: 660\u001b[0m\n",
      "     | > loss: 0.11098694801330566  (0.12847918008143697)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(11.4848, device='cuda:0')  (tensor(8.1407, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3555  (0.35520827770233154)\n",
      "     | > loader_time: 0.0017  (0.0018797318140665693)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:02:34 -- STEP: 68/204 -- GLOBAL_STEP: 680\u001b[0m\n",
      "     | > loss: 0.13694411516189575  (0.12158393257242786)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(10.9828, device='cuda:0')  (tensor(8.6586, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3549  (0.3552234172821045)\n",
      "     | > loader_time: 0.0017  (0.0018579539130715763)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:02:41 -- STEP: 88/204 -- GLOBAL_STEP: 700\u001b[0m\n",
      "     | > loss: 0.09860971570014954  (0.11817255844785404)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(9.8429, device='cuda:0')  (tensor(8.9669, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3553  (0.35523341731591657)\n",
      "     | > loader_time: 0.0017  (0.001854297789660367)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:02:48 -- STEP: 108/204 -- GLOBAL_STEP: 720\u001b[0m\n",
      "     | > loss: 0.1160498559474945  (0.11411606996423665)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(11.5391, device='cuda:0')  (tensor(9.0311, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3552  (0.35524493897402726)\n",
      "     | > loader_time: 0.0017  (0.0018506491625750506)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:02:55 -- STEP: 128/204 -- GLOBAL_STEP: 740\u001b[0m\n",
      "     | > loss: 0.1003251001238823  (0.1112708688015118)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(9.8573, device='cuda:0')  (tensor(9.1070, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3551  (0.35525133460760117)\n",
      "     | > loader_time: 0.0018  (0.0018464438617229462)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:03:02 -- STEP: 148/204 -- GLOBAL_STEP: 760\u001b[0m\n",
      "     | > loss: 0.1167488843202591  (0.10976379349626397)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(11.5462, device='cuda:0')  (tensor(9.1030, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3549  (0.35524999450992895)\n",
      "     | > loader_time: 0.0017  (0.0018402821308857685)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:03:09 -- STEP: 168/204 -- GLOBAL_STEP: 780\u001b[0m\n",
      "     | > loss: 0.09142588824033737  (0.10891603314805597)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(8.4009, device='cuda:0')  (tensor(9.1215, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3552  (0.3552383936586834)\n",
      "     | > loader_time: 0.0018  (0.001842227720078968)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:03:17 -- STEP: 188/204 -- GLOBAL_STEP: 800\u001b[0m\n",
      "     | > loss: 0.06443488597869873  (0.10800132032563076)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(4.9273, device='cuda:0')  (tensor(9.0794, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3552  (0.35524047055142993)\n",
      "     | > loader_time: 0.0016  (0.0018491618176724048)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 0.14311888813972473  (0.14311888813972473)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 0.15168948471546173  (0.15168948471546173)\n",
      "\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.0009770393371582031 \u001b[0m(-3.886222839355469e-05)\n",
      "     | > avg_loss:\u001b[91m 0.15168948471546173 \u001b[0m(+0.020236283540725708)\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 4/10\u001b[0m\n",
      " --> results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 19:03:22) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:03:25 -- STEP: 4/204 -- GLOBAL_STEP: 820\u001b[0m\n",
      "     | > loss: 0.13183631002902985  (0.12127760052680969)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(5.5254, device='cuda:0')  (tensor(6.8150, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3552  (0.3554539680480957)\n",
      "     | > loader_time: 0.0022  (0.002843201160430908)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:03:32 -- STEP: 24/204 -- GLOBAL_STEP: 840\u001b[0m\n",
      "     | > loss: 0.11082570999860764  (0.1117855239038666)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(7.6942, device='cuda:0')  (tensor(8.4783, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3553  (0.35535262028376263)\n",
      "     | > loader_time: 0.0017  (0.0018273989359537761)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:03:39 -- STEP: 44/204 -- GLOBAL_STEP: 860\u001b[0m\n",
      "     | > loss: 0.09249316155910492  (0.10602013631300493)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(5.8457, device='cuda:0')  (tensor(8.6379, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3551  (0.35530552538958465)\n",
      "     | > loader_time: 0.0017  (0.0017870881340720437)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:03:46 -- STEP: 64/204 -- GLOBAL_STEP: 880\u001b[0m\n",
      "     | > loss: 0.08937092125415802  (0.10253828368149698)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(7.7948, device='cuda:0')  (tensor(8.7046, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3549  (0.3552408069372177)\n",
      "     | > loader_time: 0.0017  (0.0017710104584693909)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:03:53 -- STEP: 84/204 -- GLOBAL_STEP: 900\u001b[0m\n",
      "     | > loss: 0.08883213251829147  (0.10067297695648104)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(7.5789, device='cuda:0')  (tensor(8.5872, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3554  (0.3552561232021877)\n",
      "     | > loader_time: 0.002  (0.001775798343476795)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:04:01 -- STEP: 104/204 -- GLOBAL_STEP: 920\u001b[0m\n",
      "     | > loss: 0.0922151505947113  (0.10218646778510168)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(6.6307, device='cuda:0')  (tensor(8.3895, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3549  (0.3552340612961696)\n",
      "     | > loader_time: 0.0016  (0.0017664753473722017)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:04:08 -- STEP: 124/204 -- GLOBAL_STEP: 940\u001b[0m\n",
      "     | > loss: 0.10027192533016205  (0.10091557804374927)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(7.1699, device='cuda:0')  (tensor(8.4871, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3549  (0.35524197547666486)\n",
      "     | > loader_time: 0.0016  (0.0017626458598721411)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:04:15 -- STEP: 144/204 -- GLOBAL_STEP: 960\u001b[0m\n",
      "     | > loss: 0.06835664808750153  (0.10001404464451806)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(5.7836, device='cuda:0')  (tensor(8.4818, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3554  (0.3552509910530514)\n",
      "     | > loader_time: 0.0018  (0.0017661650975545247)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:04:22 -- STEP: 164/204 -- GLOBAL_STEP: 980\u001b[0m\n",
      "     | > loss: 0.06424786150455475  (0.0989174425329377)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(4.0551, device='cuda:0')  (tensor(8.4093, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3555  (0.35524177842023896)\n",
      "     | > loader_time: 0.0019  (0.001763602582419791)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:04:29 -- STEP: 184/204 -- GLOBAL_STEP: 1000\u001b[0m\n",
      "     | > loss: 0.10262929648160934  (0.10167192481458187)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(6.0639, device='cuda:0')  (tensor(8.2246, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3549  (0.3552420826061913)\n",
      "     | > loader_time: 0.0017  (0.0017719346544016962)\n",
      "\n",
      "\n",
      " > CHECKPOINT : results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000/checkpoint_1000.pth\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 0.08273272961378098  (0.08273272961378098)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 0.08089230954647064  (0.08089230954647064)\n",
      "\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.0010406970977783203 \u001b[0m(+6.365776062011719e-05)\n",
      "     | > avg_loss:\u001b[92m 0.08089230954647064 \u001b[0m(-0.07079717516899109)\n",
      "\n",
      " > BEST MODEL : results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000/best_model_1020.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 5/10\u001b[0m\n",
      " --> results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 19:04:37) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:04:38 -- STEP: 0/204 -- GLOBAL_STEP: 1020\u001b[0m\n",
      "     | > loss: 0.10482945293188095  (0.10482945293188095)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(2.6804, device='cuda:0')  (tensor(2.6804, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3693  (0.3692753314971924)\n",
      "     | > loader_time: 0.7478  (0.7478039264678955)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:04:45 -- STEP: 20/204 -- GLOBAL_STEP: 1040\u001b[0m\n",
      "     | > loss: 0.0837593674659729  (0.09738150388002395)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(7.2074, device='cuda:0')  (tensor(7.9127, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3552  (0.3553061723709106)\n",
      "     | > loader_time: 0.0017  (0.001590394973754883)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:04:52 -- STEP: 40/204 -- GLOBAL_STEP: 1060\u001b[0m\n",
      "     | > loss: 0.09026789665222168  (0.09229907151311637)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(7.5399, device='cuda:0')  (tensor(7.9917, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3557  (0.35534301996231077)\n",
      "     | > loader_time: 0.0018  (0.0017293930053710938)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:04:59 -- STEP: 60/204 -- GLOBAL_STEP: 1080\u001b[0m\n",
      "     | > loss: 0.08158013224601746  (0.08946072186032933)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(5.5716, device='cuda:0')  (tensor(7.6727, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3552  (0.35537146727244057)\n",
      "     | > loader_time: 0.0018  (0.0017844438552856445)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:05:06 -- STEP: 80/204 -- GLOBAL_STEP: 1100\u001b[0m\n",
      "     | > loss: 0.1026301458477974  (0.08928182693198324)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(5.6166, device='cuda:0')  (tensor(7.2670, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3556  (0.35540196597576146)\n",
      "     | > loader_time: 0.0018  (0.0018085241317749023)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:05:14 -- STEP: 100/204 -- GLOBAL_STEP: 1120\u001b[0m\n",
      "     | > loss: 0.07175314426422119  (0.09178785972297192)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(6.3557, device='cuda:0')  (tensor(6.9369, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3553  (0.3553917694091796)\n",
      "     | > loader_time: 0.0018  (0.0018255329132080078)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:05:21 -- STEP: 120/204 -- GLOBAL_STEP: 1140\u001b[0m\n",
      "     | > loss: 0.06276068091392517  (0.09159503392875194)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(6.0515, device='cuda:0')  (tensor(7.0866, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3552  (0.3553890228271484)\n",
      "     | > loader_time: 0.0018  (0.0018337309360504151)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:05:28 -- STEP: 140/204 -- GLOBAL_STEP: 1160\u001b[0m\n",
      "     | > loss: 0.08641555905342102  (0.09042858960373061)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(3.3095, device='cuda:0')  (tensor(7.0148, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.355  (0.3553843123572213)\n",
      "     | > loader_time: 0.0018  (0.0018476009368896484)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:05:35 -- STEP: 160/204 -- GLOBAL_STEP: 1180\u001b[0m\n",
      "     | > loss: 0.08188168704509735  (0.0900971120223403)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(2.9176, device='cuda:0')  (tensor(6.7982, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3553  (0.3553790211677551)\n",
      "     | > loader_time: 0.0019  (0.001860940456390381)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:05:42 -- STEP: 180/204 -- GLOBAL_STEP: 1200\u001b[0m\n",
      "     | > loss: 0.0857030600309372  (0.08976568844583302)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(6.7561, device='cuda:0')  (tensor(6.6015, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3557  (0.35537113083733435)\n",
      "     | > loader_time: 0.0018  (0.0018602530161539713)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:05:49 -- STEP: 200/204 -- GLOBAL_STEP: 1220\u001b[0m\n",
      "     | > loss: 0.10829095542430878  (0.09129175793379547)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(3.0593, device='cuda:0')  (tensor(6.4357, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3551  (0.35534302711486804)\n",
      "     | > loader_time: 0.0015  (0.0018409085273742676)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 0.08960464596748352  (0.08960464596748352)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 0.09864357858896255  (0.09864357858896255)\n",
      "\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.0010769367218017578 \u001b[0m(+3.62396240234375e-05)\n",
      "     | > avg_loss:\u001b[91m 0.09864357858896255 \u001b[0m(+0.017751269042491913)\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 6/10\u001b[0m\n",
      " --> results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 19:05:52) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:05:59 -- STEP: 16/204 -- GLOBAL_STEP: 1240\u001b[0m\n",
      "     | > loss: 0.08651475608348846  (0.08675671461969614)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(9.9361, device='cuda:0')  (tensor(6.4178, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3563  (0.3565078377723694)\n",
      "     | > loader_time: 0.002  (0.0020264387130737305)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:06:06 -- STEP: 36/204 -- GLOBAL_STEP: 1260\u001b[0m\n",
      "     | > loss: 0.0870174840092659  (0.09087831899523735)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(5.9772, device='cuda:0')  (tensor(6.9398, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3566  (0.3565631906191508)\n",
      "     | > loader_time: 0.0017  (0.001855141586727566)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:06:13 -- STEP: 56/204 -- GLOBAL_STEP: 1280\u001b[0m\n",
      "     | > loss: 0.07838831096887589  (0.08783154788294009)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(4.4178, device='cuda:0')  (tensor(7.1100, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3567  (0.35658279061317444)\n",
      "     | > loader_time: 0.0017  (0.0018107593059539795)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:06:21 -- STEP: 76/204 -- GLOBAL_STEP: 1300\u001b[0m\n",
      "     | > loss: 0.06878960132598877  (0.08523275663978173)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(1.7966, device='cuda:0')  (tensor(6.8381, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3563  (0.3565885142276162)\n",
      "     | > loader_time: 0.0017  (0.0017904074568497507)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:06:28 -- STEP: 96/204 -- GLOBAL_STEP: 1320\u001b[0m\n",
      "     | > loss: 0.09406555444002151  (0.08494895467689878)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(7.9302, device='cuda:0')  (tensor(6.9531, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3564  (0.356611467897892)\n",
      "     | > loader_time: 0.0018  (0.0017813021938006084)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:06:35 -- STEP: 116/204 -- GLOBAL_STEP: 1340\u001b[0m\n",
      "     | > loss: 0.08172117173671722  (0.08412940610713997)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(5.4511, device='cuda:0')  (tensor(6.9236, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3566  (0.35664110348142425)\n",
      "     | > loader_time: 0.0016  (0.0017879954699812264)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:06:42 -- STEP: 136/204 -- GLOBAL_STEP: 1360\u001b[0m\n",
      "     | > loss: 0.09273751080036163  (0.08330583646345663)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(9.6591, device='cuda:0')  (tensor(6.9787, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3568  (0.3566363208434161)\n",
      "     | > loader_time: 0.0017  (0.001776435795952292)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:06:49 -- STEP: 156/204 -- GLOBAL_STEP: 1380\u001b[0m\n",
      "     | > loss: 0.05501284450292587  (0.0831541522907523)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(4.5462, device='cuda:0')  (tensor(6.9712, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3563  (0.3566268086433411)\n",
      "     | > loader_time: 0.0017  (0.0017751974937243338)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:06:56 -- STEP: 176/204 -- GLOBAL_STEP: 1400\u001b[0m\n",
      "     | > loss: 0.09399864077568054  (0.08319163849492639)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(7.6509, device='cuda:0')  (tensor(6.8952, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3565  (0.3566278855909001)\n",
      "     | > loader_time: 0.0018  (0.001770163124257868)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:07:04 -- STEP: 196/204 -- GLOBAL_STEP: 1420\u001b[0m\n",
      "     | > loss: 0.08991102874279022  (0.08390580482628873)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(7.9419, device='cuda:0')  (tensor(6.8481, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3565  (0.35662092116414285)\n",
      "     | > loader_time: 0.0015  (0.001761499716311085)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 0.17199453711509705  (0.17199453711509705)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 0.1667201668024063  (0.1667201668024063)\n",
      "\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.0009844303131103516 \u001b[0m(-9.250640869140625e-05)\n",
      "     | > avg_loss:\u001b[91m 0.1667201668024063 \u001b[0m(+0.06807658821344376)\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 7/10\u001b[0m\n",
      " --> results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 19:07:08) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:07:13 -- STEP: 12/204 -- GLOBAL_STEP: 1440\u001b[0m\n",
      "     | > loss: 0.08481848239898682  (0.11590468945602576)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(6.4452, device='cuda:0')  (tensor(7.7317, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3564  (0.3567193349202474)\n",
      "     | > loader_time: 0.0013  (0.001777668793996175)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:07:20 -- STEP: 32/204 -- GLOBAL_STEP: 1460\u001b[0m\n",
      "     | > loss: 0.08079836517572403  (0.09734085318632424)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(4.3878, device='cuda:0')  (tensor(7.4757, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3565  (0.35667482018470764)\n",
      "     | > loader_time: 0.0018  (0.0018031895160675049)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:07:27 -- STEP: 52/204 -- GLOBAL_STEP: 1480\u001b[0m\n",
      "     | > loss: 0.09523528814315796  (0.09017777586212525)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(8.8404, device='cuda:0')  (tensor(7.2620, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3566  (0.356643782212184)\n",
      "     | > loader_time: 0.0017  (0.0018086891907912034)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:07:34 -- STEP: 72/204 -- GLOBAL_STEP: 1500\u001b[0m\n",
      "     | > loss: 0.06528819352388382  (0.08765255447684063)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(5.5234, device='cuda:0')  (tensor(7.1146, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3564  (0.356624123122957)\n",
      "     | > loader_time: 0.002  (0.0018111566702524822)\n",
      "\n",
      "\n",
      " > CHECKPOINT : results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000/checkpoint_1500.pth\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:07:42 -- STEP: 92/204 -- GLOBAL_STEP: 1520\u001b[0m\n",
      "     | > loss: 0.09621800482273102  (0.08714449231553337)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(4.0349, device='cuda:0')  (tensor(7.0326, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3567  (0.356617606204489)\n",
      "     | > loader_time: 0.0018  (0.001809871715048085)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:07:49 -- STEP: 112/204 -- GLOBAL_STEP: 1540\u001b[0m\n",
      "     | > loss: 0.10635927319526672  (0.08578306626129363)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(11.5271, device='cuda:0')  (tensor(6.7062, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3564  (0.35660471873623983)\n",
      "     | > loader_time: 0.0018  (0.0018051330532346452)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:07:56 -- STEP: 132/204 -- GLOBAL_STEP: 1560\u001b[0m\n",
      "     | > loss: 0.0956072211265564  (0.08448534924536943)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(7.0811, device='cuda:0')  (tensor(6.4321, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3567  (0.35660103234377777)\n",
      "     | > loader_time: 0.0018  (0.001802587147915002)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:08:03 -- STEP: 152/204 -- GLOBAL_STEP: 1580\u001b[0m\n",
      "     | > loss: 0.07916109263896942  (0.08455732055498581)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(8.0355, device='cuda:0')  (tensor(6.5188, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3565  (0.3565990846408041)\n",
      "     | > loader_time: 0.0018  (0.0017909078221572073)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:08:10 -- STEP: 172/204 -- GLOBAL_STEP: 1600\u001b[0m\n",
      "     | > loss: 0.07781320065259933  (0.08460598707545636)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(4.6945, device='cuda:0')  (tensor(6.4984, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3565  (0.3565940593564233)\n",
      "     | > loader_time: 0.0019  (0.0017912124478539756)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:08:18 -- STEP: 192/204 -- GLOBAL_STEP: 1620\u001b[0m\n",
      "     | > loss: 0.11407536268234253  (0.0846250201575458)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(11.2199, device='cuda:0')  (tensor(6.4016, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3562  (0.35658762107292813)\n",
      "     | > loader_time: 0.0015  (0.0017854683101177216)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 0.09480755776166916  (0.09480755776166916)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 0.09214059263467789  (0.09214059263467789)\n",
      "\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.0012238025665283203 \u001b[0m(+0.00023937225341796875)\n",
      "     | > avg_loss:\u001b[92m 0.09214059263467789 \u001b[0m(-0.07457957416772842)\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 8/10\u001b[0m\n",
      " --> results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 19:08:23) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:08:27 -- STEP: 8/204 -- GLOBAL_STEP: 1640\u001b[0m\n",
      "     | > loss: 0.07496975362300873  (0.07426689239218831)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(4.7400, device='cuda:0')  (tensor(6.6217, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3568  (0.35663560032844543)\n",
      "     | > loader_time: 0.0028  (0.0024143755435943604)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:08:34 -- STEP: 28/204 -- GLOBAL_STEP: 1660\u001b[0m\n",
      "     | > loss: 0.0578589104115963  (0.07269093673676251)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(3.9127, device='cuda:0')  (tensor(5.4395, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3563  (0.35662422861371723)\n",
      "     | > loader_time: 0.0017  (0.0019179497446332658)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:08:41 -- STEP: 48/204 -- GLOBAL_STEP: 1680\u001b[0m\n",
      "     | > loss: 0.10257569700479507  (0.0776603097716967)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(10.7225, device='cuda:0')  (tensor(5.7245, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3568  (0.3566409796476364)\n",
      "     | > loader_time: 0.0021  (0.001855770746866862)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:08:48 -- STEP: 68/204 -- GLOBAL_STEP: 1700\u001b[0m\n",
      "     | > loss: 0.06039366498589516  (0.07867592315682591)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(6.1027, device='cuda:0')  (tensor(6.1093, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3563  (0.3566398866036359)\n",
      "     | > loader_time: 0.0018  (0.0018442203016842112)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:08:56 -- STEP: 88/204 -- GLOBAL_STEP: 1720\u001b[0m\n",
      "     | > loss: 0.058560971170663834  (0.07690532374280418)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(3.7626, device='cuda:0')  (tensor(6.0740, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3568  (0.356653395024213)\n",
      "     | > loader_time: 0.0018  (0.001832271164113825)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:09:03 -- STEP: 108/204 -- GLOBAL_STEP: 1740\u001b[0m\n",
      "     | > loss: 0.09046520292758942  (0.07716381456702948)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(1.6213, device='cuda:0')  (tensor(6.0712, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3568  (0.3566652558468007)\n",
      "     | > loader_time: 0.0019  (0.0018260191988061975)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:09:10 -- STEP: 128/204 -- GLOBAL_STEP: 1760\u001b[0m\n",
      "     | > loss: 0.11531424522399902  (0.07941256617777981)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(6.3134, device='cuda:0')  (tensor(5.9387, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3568  (0.35666223801672464)\n",
      "     | > loader_time: 0.0018  (0.0018157251179218292)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:09:17 -- STEP: 148/204 -- GLOBAL_STEP: 1780\u001b[0m\n",
      "     | > loss: 0.08538921177387238  (0.07962394130693093)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(4.0565, device='cuda:0')  (tensor(5.9931, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3566  (0.35666253437867046)\n",
      "     | > loader_time: 0.0016  (0.0018034077979422905)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:09:24 -- STEP: 168/204 -- GLOBAL_STEP: 1800\u001b[0m\n",
      "     | > loss: 0.0781155377626419  (0.07880567670578047)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(6.4576, device='cuda:0')  (tensor(6.0053, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3567  (0.35666453696432576)\n",
      "     | > loader_time: 0.0019  (0.0017951386315482004)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:09:32 -- STEP: 188/204 -- GLOBAL_STEP: 1820\u001b[0m\n",
      "     | > loss: 0.07805399596691132  (0.07816133881978528)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(3.0883, device='cuda:0')  (tensor(5.9571, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3563  (0.3566594948159888)\n",
      "     | > loader_time: 0.0015  (0.0017966108119234125)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 0.0680905282497406  (0.0680905282497406)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 0.09941378235816956  (0.09941378235816956)\n",
      "\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.0010268688201904297 \u001b[0m(-0.00019693374633789062)\n",
      "     | > avg_loss:\u001b[91m 0.09941378235816956 \u001b[0m(+0.007273189723491669)\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 9/10\u001b[0m\n",
      " --> results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 19:09:38) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:09:41 -- STEP: 4/204 -- GLOBAL_STEP: 1840\u001b[0m\n",
      "     | > loss: 0.0906299501657486  (0.07272085454314947)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(5.7052, device='cuda:0')  (tensor(4.8504, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3564  (0.3566042184829712)\n",
      "     | > loader_time: 0.0012  (0.0015023350715637207)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:09:48 -- STEP: 24/204 -- GLOBAL_STEP: 1860\u001b[0m\n",
      "     | > loss: 0.08502517640590668  (0.07543491494531433)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(2.8370, device='cuda:0')  (tensor(5.2240, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3569  (0.35666173696517944)\n",
      "     | > loader_time: 0.0019  (0.0016509592533111572)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:09:55 -- STEP: 44/204 -- GLOBAL_STEP: 1880\u001b[0m\n",
      "     | > loss: 0.08892164379358292  (0.07424681438979776)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(8.0249, device='cuda:0')  (tensor(5.6743, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3566  (0.35666569796475517)\n",
      "     | > loader_time: 0.0017  (0.0017197457226839933)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:10:02 -- STEP: 64/204 -- GLOBAL_STEP: 1900\u001b[0m\n",
      "     | > loss: 0.06098455190658569  (0.0742470254190266)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(5.6180, device='cuda:0')  (tensor(5.8203, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3565  (0.3566761650145055)\n",
      "     | > loader_time: 0.002  (0.001754865050315857)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:10:10 -- STEP: 84/204 -- GLOBAL_STEP: 1920\u001b[0m\n",
      "     | > loss: 0.05929378792643547  (0.07335090025195051)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(2.4880, device='cuda:0')  (tensor(5.8039, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3566  (0.3566765047255019)\n",
      "     | > loader_time: 0.0018  (0.00176271086647397)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:10:17 -- STEP: 104/204 -- GLOBAL_STEP: 1940\u001b[0m\n",
      "     | > loss: 0.07296694070100784  (0.07482083883279789)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(6.9839, device='cuda:0')  (tensor(5.6897, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3569  (0.35668012729057913)\n",
      "     | > loader_time: 0.0016  (0.0017756040279681866)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:10:24 -- STEP: 124/204 -- GLOBAL_STEP: 1960\u001b[0m\n",
      "     | > loss: 0.10081107914447784  (0.0774959920034293)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(7.7793, device='cuda:0')  (tensor(5.7445, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3565  (0.35668379068374645)\n",
      "     | > loader_time: 0.0018  (0.0017863935039889429)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:10:31 -- STEP: 144/204 -- GLOBAL_STEP: 1980\u001b[0m\n",
      "     | > loss: 0.07232090085744858  (0.07762284178493747)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(6.5649, device='cuda:0')  (tensor(5.6910, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.357  (0.3566843999756708)\n",
      "     | > loader_time: 0.0017  (0.0017968714237213135)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:10:38 -- STEP: 164/204 -- GLOBAL_STEP: 2000\u001b[0m\n",
      "     | > loss: 0.061402615159749985  (0.07751959148885272)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(0.6655, device='cuda:0')  (tensor(5.6477, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3566  (0.3566867941763343)\n",
      "     | > loader_time: 0.0019  (0.001803597299064078)\n",
      "\n",
      "\n",
      " > CHECKPOINT : results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000/checkpoint_2000.pth\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 19:10:46 -- STEP: 184/204 -- GLOBAL_STEP: 2020\u001b[0m\n",
      "     | > loss: 0.07301101088523865  (0.07677344850304978)\n",
      "     | > amp_scaler: 16384.0  (9037.91304347826)\n",
      "     | > grad_norm: tensor(6.5986, device='cuda:0')  (tensor(5.6156, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3566  (0.3566870495029118)\n",
      "     | > loader_time: 0.0019  (0.001805936512739762)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 0.06685023009777069  (0.06685023009777069)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 0.07616928964853287  (0.07616928964853287)\n",
      "\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.0010442733764648438 \u001b[0m(+1.7404556274414062e-05)\n",
      "     | > avg_loss:\u001b[92m 0.07616928964853287 \u001b[0m(-0.023244492709636688)\n",
      "\n",
      " > BEST MODEL : results/ft_wavegrad_ljspeech/run-May-24-2025_06+58PM-0000000/best_model_2040.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.vocoder.configs import WavegradConfig\n",
    "from TTS.vocoder.datasets.preprocess import load_wav_data\n",
    "from TTS.vocoder.models.wavegrad import Wavegrad\n",
    "\n",
    "output_path = 'results/ft_wavegrad_ljspeech'\n",
    "\n",
    "config = WavegradConfig(\n",
    "    batch_size=64,                # Aggressive; safe for 24GB VRAM\n",
    "    eval_batch_size=32,\n",
    "    num_loader_workers=8,         # Use multiple CPU threads for I/O\n",
    "    num_eval_loader_workers=4,\n",
    "    run_eval=True,\n",
    "    test_delay_epochs=5,\n",
    "    epochs=10,                  # Full training cycle\n",
    "    seq_len=16384,                # Very long sequences = better audio quality\n",
    "    pad_short=3000,\n",
    "    use_noise_augment=True,\n",
    "    eval_split_size=100,          # Larger eval set for robust monitoring\n",
    "    print_step=20,\n",
    "    print_eval=True,\n",
    "    mixed_precision=True,         # Essential for best performance on 4090\n",
    "    data_path=\"LJSpeech-1.1/wavs/\",\n",
    "    output_path=output_path,\n",
    ")\n",
    "\n",
    "# init audio processor\n",
    "ap = AudioProcessor(**config.audio.to_dict())\n",
    "\n",
    "# load training samples\n",
    "eval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n",
    "\n",
    "# init model\n",
    "model = Wavegrad(config)\n",
    "\n",
    "# init the trainer and ğŸš€\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(),\n",
    "    config,\n",
    "    output_path,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    "    training_assets={\"audio_processor\": ap},\n",
    ")\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415e93ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17468/2655252565.py:39: DeprecationWarning: `cumproduct` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `cumprod` instead.\n",
      "  model = Wavernn(config)\n",
      "fatal: not a git repository (or any parent up to mount point /mnt)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      " > Training Environment:\n",
      " | > Backend: Torch\n",
      " | > Mixed precision: True\n",
      " | > Precision: fp16\n",
      " | > Current device: 0\n",
      " | > Num. of GPUs: 1\n",
      " | > Num. of CPUs: 24\n",
      " | > Num. of Torch Threads: 12\n",
      " | > Torch seed: 54321\n",
      " | > Torch CUDNN: True\n",
      " | > Torch CUDNN deterministic: False\n",
      " | > Torch CUDNN benchmark: False\n",
      " | > Torch TF32 MatMul: False\n",
      " > Start Tensorboard: tensorboard --logdir=results/ft_wavernn_ljspeech/run-May-24-2025_08+06PM-0000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:None\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:45\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:None\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:45\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Storage1/grozavu/conversion/.venv/lib/python3.11/site-packages/trainer/trainer.py:552: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "\n",
      " > Model has 4233673 parameters\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/10\u001b[0m\n",
      " --> results/ft_wavernn_ljspeech/run-May-24-2025_08+06PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 20:06:48) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:06:58 -- STEP: 0/103 -- GLOBAL_STEP: 0\u001b[0m\n",
      "     | > loss: 11.831850051879883  (11.831850051879883)\n",
      "     | > amp_scaler: 65536.0  (65536.0)\n",
      "     | > grad_norm: tensor(2.1972, device='cuda:0')  (tensor(2.1972, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 5.3197  (5.31970739364624)\n",
      "     | > loader_time: 5.1933  (5.19328498840332)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:07:11 -- STEP: 25/103 -- GLOBAL_STEP: 25\u001b[0m\n",
      "     | > loss: 10.36344051361084  (10.679218139648437)\n",
      "     | > amp_scaler: 2048.0  (23756.8)\n",
      "     | > grad_norm: tensor(70.6353, device='cuda:0')  (tensor(36.1995, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4642  (0.4225919246673584)\n",
      "     | > loader_time: 0.0885  (0.09327131271362306)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:07:25 -- STEP: 50/103 -- GLOBAL_STEP: 50\u001b[0m\n",
      "     | > loss: 9.550715446472168  (10.240336818695063)\n",
      "     | > amp_scaler: 2048.0  (12902.4)\n",
      "     | > grad_norm: tensor(183.4649, device='cuda:0')  (tensor(59.5530, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3724  (0.4204805040359497)\n",
      "     | > loader_time: 0.0981  (0.11928605556488037)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:07:43 -- STEP: 75/103 -- GLOBAL_STEP: 75\u001b[0m\n",
      "     | > loss: 9.205504417419434  (9.944698994954424)\n",
      "     | > amp_scaler: 2048.0  (9284.266666666666)\n",
      "     | > grad_norm: tensor(50.2015, device='cuda:0')  (tensor(74.0092, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4296  (0.43670598983764647)\n",
      "     | > loader_time: 0.0917  (0.15401294390360512)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:07:55 -- STEP: 100/103 -- GLOBAL_STEP: 100\u001b[0m\n",
      "     | > loss: 8.95341682434082  (9.76660045623779)\n",
      "     | > amp_scaler: 2048.0  (7475.199999999999)\n",
      "     | > grad_norm: tensor(50.4533, device='cuda:0')  (tensor(84.4324, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.0966  (0.41139486074447634)\n",
      "     | > loader_time: 0.0004  (0.15278246641159057)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 8.925863265991211  (8.925863265991211)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 8.735939979553223  (8.735939979553223)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss: 9.402477264404297  (9.06920862197876)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss: 9.389473915100098  (9.175963719685873)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss: 8.817486763000488  (9.086344480514526)\n",
      "\n",
      "\u001b[1m   --> STEP: 5\u001b[0m\n",
      "     | > loss: 9.064631462097168  (9.082001876831054)\n",
      "\n",
      "\u001b[1m   --> STEP: 6\u001b[0m\n",
      "     | > loss: 8.610086441040039  (9.003349304199219)\n",
      "\n",
      "\u001b[1m   --> STEP: 7\u001b[0m\n",
      "     | > loss: 9.011801719665527  (9.004556792122978)\n",
      "\n",
      "\u001b[1m   --> STEP: 8\u001b[0m\n",
      "     | > loss: 9.30990982055664  (9.042725920677185)\n",
      "\n",
      "\u001b[1m   --> STEP: 9\u001b[0m\n",
      "     | > loss: 9.007081985473633  (9.038765483432346)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [!] Instance is too short! : LJSpeech-1.1/wavs/LJ017-0228.wav\n",
      "156000/157300 -- batch_size: 13 -- gen_rate: 40.5 kHz -- x_realtime: 1.8  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time: 0.00010829501681857639 \u001b[0m(+0)\n",
      "     | > avg_loss: 9.038765483432346 \u001b[0m(+0)\n",
      "\n",
      " > BEST MODEL : results/ft_wavernn_ljspeech/run-May-24-2025_08+06PM-0000000/best_model_103.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 1/10\u001b[0m\n",
      " --> results/ft_wavernn_ljspeech/run-May-24-2025_08+06PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 20:08:01) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:08:21 -- STEP: 22/103 -- GLOBAL_STEP: 125\u001b[0m\n",
      "     | > loss: 9.188130378723145  (9.443207784132523)\n",
      "     | > amp_scaler: 512.0  (558.5454545454546)\n",
      "     | > grad_norm: tensor(105.0811, device='cuda:0')  (tensor(175.0742, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4562  (0.49201423471624195)\n",
      "     | > loader_time: 0.1126  (0.1704457781531594)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:08:37 -- STEP: 47/103 -- GLOBAL_STEP: 150\u001b[0m\n",
      "     | > loss: 9.734430313110352  (9.326689091134575)\n",
      "     | > amp_scaler: 512.0  (533.787234042553)\n",
      "     | > grad_norm: tensor(225.5271, device='cuda:0')  (tensor(166.5952, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4577  (0.47164836335689464)\n",
      "     | > loader_time: 0.1925  (0.1783675031459078)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:08:53 -- STEP: 72/103 -- GLOBAL_STEP: 175\u001b[0m\n",
      "     | > loss: 9.409708023071289  (9.278715279367235)\n",
      "     | > amp_scaler: 512.0  (526.2222222222221)\n",
      "     | > grad_norm: tensor(148.4149, device='cuda:0')  (tensor(152.3200, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4411  (0.46014934447076583)\n",
      "     | > loader_time: 0.1297  (0.19188853104909262)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:09:07 -- STEP: 97/103 -- GLOBAL_STEP: 200\u001b[0m\n",
      "     | > loss: 9.374873161315918  (9.311961203506318)\n",
      "     | > amp_scaler: 512.0  (522.5567010309278)\n",
      "     | > grad_norm: tensor(91.9126, device='cuda:0')  (tensor(141.6607, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.1658  (0.43895973372705205)\n",
      "     | > loader_time: 0.0426  (0.18307451857733972)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 9.483691215515137  (9.483691215515137)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 9.203161239624023  (9.203161239624023)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss: 9.033854484558105  (9.118507862091064)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss: 9.474459648132324  (9.23715845743815)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss: 9.98091983795166  (9.423098802566528)\n",
      "\n",
      "\u001b[1m   --> STEP: 5\u001b[0m\n",
      "     | > loss: 9.439908027648926  (9.426460647583008)\n",
      "\n",
      "\u001b[1m   --> STEP: 6\u001b[0m\n",
      "     | > loss: 8.82789134979248  (9.326699097951254)\n",
      "\n",
      "\u001b[1m   --> STEP: 7\u001b[0m\n",
      "     | > loss: 9.510441780090332  (9.35294805254255)\n",
      "\n",
      "\u001b[1m   --> STEP: 8\u001b[0m\n",
      "     | > loss: 9.582730293273926  (9.381670832633972)\n",
      "\n",
      "\u001b[1m   --> STEP: 9\u001b[0m\n",
      "     | > loss: 8.966437339782715  (9.33553377787272)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [!] Instance is too short! : LJSpeech-1.1/wavs/LJ017-0228.wav\n",
      "156000/157300 -- batch_size: 13 -- gen_rate: 40.7 kHz -- x_realtime: 1.8  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.00011838807000054253 \u001b[0m(+1.0093053181966141e-05)\n",
      "     | > avg_loss:\u001b[91m 9.33553377787272 \u001b[0m(+0.29676829444037445)\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 2/10\u001b[0m\n",
      " --> results/ft_wavernn_ljspeech/run-May-24-2025_08+06PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 20:09:12) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:09:31 -- STEP: 19/103 -- GLOBAL_STEP: 225\u001b[0m\n",
      "     | > loss: 9.061210632324219  (9.223846837093955)\n",
      "     | > amp_scaler: 512.0  (512.0)\n",
      "     | > grad_norm: tensor(94.6936, device='cuda:0')  (tensor(150.4175, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3609  (0.4330350599790874)\n",
      "     | > loader_time: 1.4758  (0.2023513442591617)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:09:48 -- STEP: 44/103 -- GLOBAL_STEP: 250\u001b[0m\n",
      "     | > loss: 9.244539260864258  (9.233693079514937)\n",
      "     | > amp_scaler: 512.0  (512.0)\n",
      "     | > grad_norm: tensor(184.6055, device='cuda:0')  (tensor(140.2444, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.5118  (0.43918345191261987)\n",
      "     | > loader_time: 0.1221  (0.224787560376254)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:10:02 -- STEP: 69/103 -- GLOBAL_STEP: 275\u001b[0m\n",
      "     | > loss: 8.96158504486084  (9.178129942520808)\n",
      "     | > amp_scaler: 512.0  (512.0)\n",
      "     | > grad_norm: tensor(113.6084, device='cuda:0')  (tensor(139.7455, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3874  (0.4450765761776247)\n",
      "     | > loader_time: 0.1184  (0.1904077080712802)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:10:17 -- STEP: 94/103 -- GLOBAL_STEP: 300\u001b[0m\n",
      "     | > loss: 9.195155143737793  (9.205798910019247)\n",
      "     | > amp_scaler: 512.0  (512.0)\n",
      "     | > grad_norm: tensor(234.0354, device='cuda:0')  (tensor(164.9946, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.2599  (0.42539135192302946)\n",
      "     | > loader_time: 0.0581  (0.19922434269113742)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 9.068307876586914  (9.068307876586914)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 9.414385795593262  (9.414385795593262)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss: 8.281184196472168  (8.847784996032715)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss: 9.063385009765625  (8.919651667277018)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss: 9.408088684082031  (9.041760921478271)\n",
      "\n",
      "\u001b[1m   --> STEP: 5\u001b[0m\n",
      "     | > loss: 9.281966209411621  (9.08980197906494)\n",
      "\n",
      "\u001b[1m   --> STEP: 6\u001b[0m\n",
      "     | > loss: 8.983738899230957  (9.072124799092611)\n",
      "\n",
      "\u001b[1m   --> STEP: 7\u001b[0m\n",
      "     | > loss: 8.753890037536621  (9.026662690298897)\n",
      "\n",
      "\u001b[1m   --> STEP: 8\u001b[0m\n",
      "     | > loss: 8.935233116149902  (9.015233993530273)\n",
      "\n",
      "\u001b[1m   --> STEP: 9\u001b[0m\n",
      "     | > loss: 9.643281936645508  (9.085017098320854)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [!] Instance is too short! : LJSpeech-1.1/wavs/LJ017-0228.wav\n",
      "156000/157300 -- batch_size: 13 -- gen_rate: 41.0 kHz -- x_realtime: 1.9  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.00012376573350694444 \u001b[0m(+5.377663506401907e-06)\n",
      "     | > avg_loss:\u001b[92m 9.085017098320854 \u001b[0m(-0.2505166795518665)\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 3/10\u001b[0m\n",
      " --> results/ft_wavernn_ljspeech/run-May-24-2025_08+06PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 20:10:24) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:10:40 -- STEP: 16/103 -- GLOBAL_STEP: 325\u001b[0m\n",
      "     | > loss: 8.926268577575684  (9.263236105442047)\n",
      "     | > amp_scaler: 512.0  (512.0)\n",
      "     | > grad_norm: tensor(82.6857, device='cuda:0')  (tensor(186.4271, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4854  (0.5246416479349136)\n",
      "     | > loader_time: 0.1226  (0.1526537388563156)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:10:57 -- STEP: 41/103 -- GLOBAL_STEP: 350\u001b[0m\n",
      "     | > loss: 9.287164688110352  (9.172030634996368)\n",
      "     | > amp_scaler: 512.0  (512.0)\n",
      "     | > grad_norm: tensor(250.0444, device='cuda:0')  (tensor(174.2277, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4579  (0.48094101068450185)\n",
      "     | > loader_time: 0.1243  (0.19804912660180068)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:11:14 -- STEP: 66/103 -- GLOBAL_STEP: 375\u001b[0m\n",
      "     | > loss: 8.88852310180664  (9.206270810329555)\n",
      "     | > amp_scaler: 512.0  (512.0)\n",
      "     | > grad_norm: tensor(154.0385, device='cuda:0')  (tensor(178.1092, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4798  (0.4867714029369932)\n",
      "     | > loader_time: 0.106  (0.1932772397994995)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:11:29 -- STEP: 91/103 -- GLOBAL_STEP: 400\u001b[0m\n",
      "     | > loss: 9.622830390930176  (9.212299189724769)\n",
      "     | > amp_scaler: 512.0  (512.0)\n",
      "     | > grad_norm: tensor(282.8582, device='cuda:0')  (tensor(173.4005, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3036  (0.4716885168473799)\n",
      "     | > loader_time: 0.08  (0.18214293888636998)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 9.570385932922363  (9.570385932922363)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 9.839385032653809  (9.839385032653809)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss: 9.356363296508789  (9.597874164581299)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss: 9.692156791687012  (9.62930170694987)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss: 9.378796577453613  (9.566675424575806)\n",
      "\n",
      "\u001b[1m   --> STEP: 5\u001b[0m\n",
      "     | > loss: 9.65219783782959  (9.583779907226562)\n",
      "\n",
      "\u001b[1m   --> STEP: 6\u001b[0m\n",
      "     | > loss: 9.452646255493164  (9.56192429860433)\n",
      "\n",
      "\u001b[1m   --> STEP: 7\u001b[0m\n",
      "     | > loss: 9.729883193969727  (9.585918426513672)\n",
      "\n",
      "\u001b[1m   --> STEP: 8\u001b[0m\n",
      "     | > loss: 9.914700508117676  (9.627016186714172)\n",
      "\n",
      "\u001b[1m   --> STEP: 9\u001b[0m\n",
      "     | > loss: 8.513412475585938  (9.503282441033257)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [!] Instance is too short! : LJSpeech-1.1/wavs/LJ017-0228.wav\n",
      "156000/157300 -- batch_size: 13 -- gen_rate: 43.0 kHz -- x_realtime: 2.0  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.0001309182908799913 \u001b[0m(+7.152557373046875e-06)\n",
      "     | > avg_loss:\u001b[91m 9.503282441033257 \u001b[0m(+0.41826534271240234)\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 4/10\u001b[0m\n",
      " --> results/ft_wavernn_ljspeech/run-May-24-2025_08+06PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 20:11:36) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:11:49 -- STEP: 13/103 -- GLOBAL_STEP: 425\u001b[0m\n",
      "     | > loss: 9.084779739379883  (9.238163434542143)\n",
      "     | > amp_scaler: 512.0  (512.0)\n",
      "     | > grad_norm: tensor(77.7895, device='cuda:0')  (tensor(160.7676, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4694  (0.5059826190655048)\n",
      "     | > loader_time: 0.1113  (0.12847097103412336)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:12:05 -- STEP: 38/103 -- GLOBAL_STEP: 450\u001b[0m\n",
      "     | > loss: 8.830370903015137  (9.071522461740598)\n",
      "     | > amp_scaler: 512.0  (512.0)\n",
      "     | > grad_norm: tensor(128.2340, device='cuda:0')  (tensor(145.8660, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4537  (0.4830055738750257)\n",
      "     | > loader_time: 0.132  (0.15392839281182544)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:12:23 -- STEP: 63/103 -- GLOBAL_STEP: 475\u001b[0m\n",
      "     | > loss: 8.805415153503418  (9.091747995406864)\n",
      "     | > amp_scaler: 512.0  (512.0)\n",
      "     | > grad_norm: tensor(63.8109, device='cuda:0')  (tensor(140.1360, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4687  (0.4516758691696894)\n",
      "     | > loader_time: 0.1179  (0.20758946736653647)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:12:38 -- STEP: 88/103 -- GLOBAL_STEP: 500\u001b[0m\n",
      "     | > loss: 9.139042854309082  (9.160984700376337)\n",
      "     | > amp_scaler: 512.0  (512.0)\n",
      "     | > grad_norm: tensor(100.7699, device='cuda:0')  (tensor(166.5910, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.3242  (0.4521559774875641)\n",
      "     | > loader_time: 0.1058  (0.1897637004202063)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 8.379137992858887  (8.379137992858887)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 9.161688804626465  (9.161688804626465)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss: 9.36330795288086  (9.262498378753662)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss: 8.662619590759277  (9.062538782755533)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss: 9.655436515808105  (9.210763216018677)\n",
      "\n",
      "\u001b[1m   --> STEP: 5\u001b[0m\n",
      "     | > loss: 9.263641357421875  (9.221338844299316)\n",
      "\n",
      "\u001b[1m   --> STEP: 6\u001b[0m\n",
      "     | > loss: 9.111607551574707  (9.203050295511881)\n",
      "\n",
      "\u001b[1m   --> STEP: 7\u001b[0m\n",
      "     | > loss: 8.230735778808594  (9.064148221697126)\n",
      "\n",
      "\u001b[1m   --> STEP: 8\u001b[0m\n",
      "     | > loss: 7.872288703918457  (8.915165781974792)\n",
      "\n",
      "\u001b[1m   --> STEP: 9\u001b[0m\n",
      "     | > loss: 7.5171380043029785  (8.75982936223348)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [!] Instance is too short! : LJSpeech-1.1/wavs/LJ017-0228.wav\n",
      "156000/157300 -- batch_size: 13 -- gen_rate: 41.2 kHz -- x_realtime: 1.9  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.00010959307352701823 \u001b[0m(-2.132521735297308e-05)\n",
      "     | > avg_loss:\u001b[92m 8.75982936223348 \u001b[0m(-0.7434530787997762)\n",
      "\n",
      " > BEST MODEL : results/ft_wavernn_ljspeech/run-May-24-2025_08+06PM-0000000/best_model_515.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 5/10\u001b[0m\n",
      " --> results/ft_wavernn_ljspeech/run-May-24-2025_08+06PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 20:12:47) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:12:59 -- STEP: 10/103 -- GLOBAL_STEP: 525\u001b[0m\n",
      "     | > loss: 8.882606506347656  (8.889536476135254)\n",
      "     | > amp_scaler: 512.0  (512.0)\n",
      "     | > grad_norm: tensor(173.5276, device='cuda:0')  (tensor(185.5352, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4555  (0.5005568742752076)\n",
      "     | > loader_time: 0.0915  (0.10104310512542725)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:13:15 -- STEP: 35/103 -- GLOBAL_STEP: 550\u001b[0m\n",
      "     | > loss: 9.630229949951172  (9.238997050694056)\n",
      "     | > amp_scaler: 512.0  (512.0)\n",
      "     | > grad_norm: tensor(156.9650, device='cuda:0')  (tensor(220.6050, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.2903  (0.46620336941310336)\n",
      "     | > loader_time: 0.4565  (0.1498454979487828)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:13:32 -- STEP: 60/103 -- GLOBAL_STEP: 575\u001b[0m\n",
      "     | > loss: 9.215597152709961  (9.234301757812501)\n",
      "     | > amp_scaler: 512.0  (512.0)\n",
      "     | > grad_norm: tensor(130.3958, device='cuda:0')  (tensor(189.9705, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.5275  (0.4561760902404785)\n",
      "     | > loader_time: 1.6222  (0.19807719389597575)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:13:49 -- STEP: 85/103 -- GLOBAL_STEP: 600\u001b[0m\n",
      "     | > loss: 9.831806182861328  (9.258269646588493)\n",
      "     | > amp_scaler: 512.0  (512.0)\n",
      "     | > grad_norm: tensor(262.4052, device='cuda:0')  (tensor(188.6662, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4075  (0.44838765088249655)\n",
      "     | > loader_time: 0.0975  (0.21040694292853857)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 8.100703239440918  (8.100703239440918)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 8.655817985534668  (8.655817985534668)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss: 9.350723266601562  (9.003270626068115)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss: 9.332989692687988  (9.113176981608072)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss: 9.38523006439209  (9.181190252304077)\n",
      "\n",
      "\u001b[1m   --> STEP: 5\u001b[0m\n",
      "     | > loss: 9.56209659576416  (9.257371520996093)\n",
      "\n",
      "\u001b[1m   --> STEP: 6\u001b[0m\n",
      "     | > loss: 8.360422134399414  (9.107879956563314)\n",
      "\n",
      "\u001b[1m   --> STEP: 7\u001b[0m\n",
      "     | > loss: 8.20837116241455  (8.979378700256348)\n",
      "\n",
      "\u001b[1m   --> STEP: 8\u001b[0m\n",
      "     | > loss: 8.24870491027832  (8.888044476509094)\n",
      "\n",
      "\u001b[1m   --> STEP: 9\u001b[0m\n",
      "     | > loss: 7.7876296043396  (8.76577615737915)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [!] Instance is too short! : LJSpeech-1.1/wavs/LJ017-0228.wav\n",
      "156000/157300 -- batch_size: 13 -- gen_rate: 40.9 kHz -- x_realtime: 1.9  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.00010381804572211372 \u001b[0m(-5.775027804904515e-06)\n",
      "     | > avg_loss:\u001b[91m 8.76577615737915 \u001b[0m(+0.005946795145669981)\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 6/10\u001b[0m\n",
      " --> results/ft_wavernn_ljspeech/run-May-24-2025_08+06PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 20:13:59) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:14:09 -- STEP: 7/103 -- GLOBAL_STEP: 625\u001b[0m\n",
      "     | > loss: 9.100076675415039  (9.09012862614223)\n",
      "     | > amp_scaler: 512.0  (512.0)\n",
      "     | > grad_norm: tensor(191.5706, device='cuda:0')  (tensor(264.5895, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.5604  (0.5131284509386335)\n",
      "     | > loader_time: 0.1014  (0.0888469900403704)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:14:27 -- STEP: 32/103 -- GLOBAL_STEP: 650\u001b[0m\n",
      "     | > loss: 9.175254821777344  (9.3783355653286)\n",
      "     | > amp_scaler: 512.0  (512.0)\n",
      "     | > grad_norm: tensor(125.9320, device='cuda:0')  (tensor(267.1367, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4796  (0.4891297593712807)\n",
      "     | > loader_time: 0.6852  (0.19490785896778107)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:14:44 -- STEP: 57/103 -- GLOBAL_STEP: 675\u001b[0m\n",
      "     | > loss: 9.443925857543945  (9.217709675169823)\n",
      "     | > amp_scaler: 512.0  (512.0)\n",
      "     | > grad_norm: tensor(598.1998, device='cuda:0')  (tensor(232.3257, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4272  (0.4822307971485874)\n",
      "     | > loader_time: 0.4326  (0.19448911098011754)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:14:59 -- STEP: 82/103 -- GLOBAL_STEP: 700\u001b[0m\n",
      "     | > loss: 9.356356620788574  (9.207770626719402)\n",
      "     | > amp_scaler: 256.0  (433.9512195121951)\n",
      "     | > grad_norm: tensor(152.1168, device='cuda:0')  (tensor(247.1673, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.5002  (0.4736535403786636)\n",
      "     | > loader_time: 0.098  (0.1828225007871302)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 10.17484188079834  (10.17484188079834)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 8.8737211227417  (8.8737211227417)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss: 9.151449203491211  (9.012585163116455)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss: 8.786053657531738  (8.937074661254883)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss: 8.45749568939209  (8.817179918289185)\n",
      "\n",
      "\u001b[1m   --> STEP: 5\u001b[0m\n",
      "     | > loss: 9.497466087341309  (8.953237152099609)\n",
      "\n",
      "\u001b[1m   --> STEP: 6\u001b[0m\n",
      "     | > loss: 9.2166166305542  (8.997133731842041)\n",
      "\n",
      "\u001b[1m   --> STEP: 7\u001b[0m\n",
      "     | > loss: 8.208497047424316  (8.884471348353795)\n",
      "\n",
      "\u001b[1m   --> STEP: 8\u001b[0m\n",
      "     | > loss: 9.001684188842773  (8.899122953414917)\n",
      "\n",
      "\u001b[1m   --> STEP: 9\u001b[0m\n",
      "     | > loss: 7.687318325042725  (8.764477994706896)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [!] Instance is too short! : LJSpeech-1.1/wavs/LJ017-0228.wav\n",
      "156000/157300 -- batch_size: 13 -- gen_rate: 41.1 kHz -- x_realtime: 1.9  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 9.240044487847222e-05 \u001b[0m(-1.1417600843641499e-05)\n",
      "     | > avg_loss:\u001b[92m 8.764477994706896 \u001b[0m(-0.001298162672254577)\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 7/10\u001b[0m\n",
      " --> results/ft_wavernn_ljspeech/run-May-24-2025_08+06PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 20:15:10) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:15:20 -- STEP: 4/103 -- GLOBAL_STEP: 725\u001b[0m\n",
      "     | > loss: 8.642287254333496  (8.809009790420532)\n",
      "     | > amp_scaler: 256.0  (256.0)\n",
      "     | > grad_norm: tensor(110.7041, device='cuda:0')  (tensor(123.7396, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.5191  (0.5123881697654724)\n",
      "     | > loader_time: 0.0908  (0.09635090827941895)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:15:37 -- STEP: 29/103 -- GLOBAL_STEP: 750\u001b[0m\n",
      "     | > loss: 9.422835350036621  (9.09821030189251)\n",
      "     | > amp_scaler: 256.0  (256.0)\n",
      "     | > grad_norm: tensor(160.7328, device='cuda:0')  (tensor(219.6180, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.5179  (0.5095299605665535)\n",
      "     | > loader_time: 0.1006  (0.15707524069424333)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:15:53 -- STEP: 54/103 -- GLOBAL_STEP: 775\u001b[0m\n",
      "     | > loss: 9.024249076843262  (9.016217390696207)\n",
      "     | > amp_scaler: 256.0  (256.0)\n",
      "     | > grad_norm: tensor(148.6553, device='cuda:0')  (tensor(178.0226, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4796  (0.49734465722684507)\n",
      "     | > loader_time: 0.0878  (0.1705597683235451)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:16:10 -- STEP: 79/103 -- GLOBAL_STEP: 800\u001b[0m\n",
      "     | > loss: 8.726694107055664  (9.06630915629713)\n",
      "     | > amp_scaler: 256.0  (256.0)\n",
      "     | > grad_norm: tensor(121.2436, device='cuda:0')  (tensor(183.0057, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.5817  (0.4688470061821274)\n",
      "     | > loader_time: 0.1053  (0.19480892676341385)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 8.44827938079834  (8.44827938079834)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 9.250685691833496  (9.250685691833496)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss: 9.322275161743164  (9.28648042678833)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss: 9.674803733825684  (9.415921529134115)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss: 9.281076431274414  (9.38221025466919)\n",
      "\n",
      "\u001b[1m   --> STEP: 5\u001b[0m\n",
      "     | > loss: 9.09630012512207  (9.325028228759766)\n",
      "\n",
      "\u001b[1m   --> STEP: 6\u001b[0m\n",
      "     | > loss: 9.103232383728027  (9.288062254587809)\n",
      "\n",
      "\u001b[1m   --> STEP: 7\u001b[0m\n",
      "     | > loss: 9.32935619354248  (9.29396138872419)\n",
      "\n",
      "\u001b[1m   --> STEP: 8\u001b[0m\n",
      "     | > loss: 9.164581298828125  (9.277788877487183)\n",
      "\n",
      "\u001b[1m   --> STEP: 9\u001b[0m\n",
      "     | > loss: 8.371268272399902  (9.177064365810818)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [!] Instance is too short! : LJSpeech-1.1/wavs/LJ017-0228.wav\n",
      "156000/157300 -- batch_size: 13 -- gen_rate: 41.1 kHz -- x_realtime: 1.9  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.00010959307352701823 \u001b[0m(+1.7192628648546014e-05)\n",
      "     | > avg_loss:\u001b[91m 9.177064365810818 \u001b[0m(+0.41258637110392193)\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 8/10\u001b[0m\n",
      " --> results/ft_wavernn_ljspeech/run-May-24-2025_08+06PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 20:16:24) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:16:30 -- STEP: 1/103 -- GLOBAL_STEP: 825\u001b[0m\n",
      "     | > loss: 10.409693717956543  (10.409693717956543)\n",
      "     | > amp_scaler: 256.0  (256.0)\n",
      "     | > grad_norm: tensor(391.4507, device='cuda:0')  (tensor(391.4507, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.5769  (0.5768787860870361)\n",
      "     | > loader_time: 0.0909  (0.09088611602783203)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:16:47 -- STEP: 26/103 -- GLOBAL_STEP: 850\u001b[0m\n",
      "     | > loss: 8.758657455444336  (9.232333696805515)\n",
      "     | > amp_scaler: 256.0  (256.0)\n",
      "     | > grad_norm: tensor(252.8665, device='cuda:0')  (tensor(151.7598, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.5093  (0.4967046609291664)\n",
      "     | > loader_time: 0.7547  (0.15032842526069057)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:17:03 -- STEP: 51/103 -- GLOBAL_STEP: 875\u001b[0m\n",
      "     | > loss: 8.647608757019043  (9.109001103569481)\n",
      "     | > amp_scaler: 256.0  (256.0)\n",
      "     | > grad_norm: tensor(61.1961, device='cuda:0')  (tensor(155.8938, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4285  (0.4693736562541887)\n",
      "     | > loader_time: 0.8529  (0.17942323404199945)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:17:18 -- STEP: 76/103 -- GLOBAL_STEP: 900\u001b[0m\n",
      "     | > loss: 8.982118606567383  (9.082426347230609)\n",
      "     | > amp_scaler: 256.0  (256.0)\n",
      "     | > grad_norm: tensor(131.8366, device='cuda:0')  (tensor(172.8816, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4379  (0.4761399469877544)\n",
      "     | > loader_time: 0.1107  (0.15967126582798208)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:17:30 -- STEP: 101/103 -- GLOBAL_STEP: 925\u001b[0m\n",
      "     | > loss: 8.643000602722168  (9.026774566952543)\n",
      "     | > amp_scaler: 256.0  (256.0)\n",
      "     | > grad_norm: tensor(75.7950, device='cuda:0')  (tensor(162.6128, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.0968  (0.4425981021163487)\n",
      "     | > loader_time: 0.0004  (0.14978720646093396)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 7.527451992034912  (7.527451992034912)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 9.700058937072754  (9.700058937072754)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss: 9.167230606079102  (9.433644771575928)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss: 8.674864768981934  (9.180718104044596)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss: 7.907147407531738  (8.862325429916382)\n",
      "\n",
      "\u001b[1m   --> STEP: 5\u001b[0m\n",
      "     | > loss: 9.392001152038574  (8.968260574340821)\n",
      "\n",
      "\u001b[1m   --> STEP: 6\u001b[0m\n",
      "     | > loss: 8.974762916564941  (8.96934429804484)\n",
      "\n",
      "\u001b[1m   --> STEP: 7\u001b[0m\n",
      "     | > loss: 10.250204086303711  (9.152324267796107)\n",
      "\n",
      "\u001b[1m   --> STEP: 8\u001b[0m\n",
      "     | > loss: 8.234084129333496  (9.037544250488281)\n",
      "\n",
      "\u001b[1m   --> STEP: 9\u001b[0m\n",
      "     | > loss: 8.932072639465332  (9.025825182596842)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [!] Instance is too short! : LJSpeech-1.1/wavs/LJ017-0228.wav\n",
      "156000/157300 -- batch_size: 13 -- gen_rate: 43.2 kHz -- x_realtime: 2.0  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.00010450681050618489 \u001b[0m(-5.086263020833342e-06)\n",
      "     | > avg_loss:\u001b[92m 9.025825182596842 \u001b[0m(-0.1512391832139759)\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 9/10\u001b[0m\n",
      " --> results/ft_wavernn_ljspeech/run-May-24-2025_08+06PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-24 20:17:35) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:17:57 -- STEP: 23/103 -- GLOBAL_STEP: 950\u001b[0m\n",
      "     | > loss: 9.140965461730957  (9.261809680772865)\n",
      "     | > amp_scaler: 256.0  (256.0)\n",
      "     | > grad_norm: tensor(64.2867, device='cuda:0')  (tensor(233.1356, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4581  (0.4955996741419253)\n",
      "     | > loader_time: 0.0966  (0.1504283676976743)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:18:13 -- STEP: 48/103 -- GLOBAL_STEP: 975\u001b[0m\n",
      "     | > loss: 8.860940933227539  (9.050653616587322)\n",
      "     | > amp_scaler: 256.0  (256.0)\n",
      "     | > grad_norm: tensor(150.8796, device='cuda:0')  (tensor(201.2847, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.4065  (0.46619660158952075)\n",
      "     | > loader_time: 0.1256  (0.1793976972500483)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:18:29 -- STEP: 73/103 -- GLOBAL_STEP: 1000\u001b[0m\n",
      "     | > loss: 8.969762802124023  (8.948700748077814)\n",
      "     | > amp_scaler: 256.0  (256.0)\n",
      "     | > grad_norm: tensor(258.2821, device='cuda:0')  (tensor(188.7088, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.5184  (0.45924184420337416)\n",
      "     | > loader_time: 0.1088  (0.1860228662621485)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-24 20:18:42 -- STEP: 98/103 -- GLOBAL_STEP: 1025\u001b[0m\n",
      "     | > loss: 8.687617301940918  (8.988903055385675)\n",
      "     | > amp_scaler: 256.0  (256.0)\n",
      "     | > grad_norm: tensor(113.2526, device='cuda:0')  (tensor(187.4662, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.0965  (0.4228925583313923)\n",
      "     | > loader_time: 0.0004  (0.18728027295093147)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss: 9.033027648925781  (9.033027648925781)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss: 8.635257720947266  (8.635257720947266)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss: 10.302396774291992  (9.468827247619629)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss: 8.663688659667969  (9.200447718302408)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss: 8.602986335754395  (9.051082372665405)\n",
      "\n",
      "\u001b[1m   --> STEP: 5\u001b[0m\n",
      "     | > loss: 8.93798828125  (9.028463554382324)\n",
      "\n",
      "\u001b[1m   --> STEP: 6\u001b[0m\n",
      "     | > loss: 8.71363353729248  (8.97599188486735)\n",
      "\n",
      "\u001b[1m   --> STEP: 7\u001b[0m\n",
      "     | > loss: 8.859118461608887  (8.959295681544713)\n",
      "\n",
      "\u001b[1m   --> STEP: 8\u001b[0m\n",
      "     | > loss: 9.034415245056152  (8.968685626983643)\n",
      "\n",
      "\u001b[1m   --> STEP: 9\u001b[0m\n",
      "     | > loss: 9.261137962341309  (9.001180330912272)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [!] Instance is too short! : LJSpeech-1.1/wavs/LJ017-0228.wav\n",
      "156000/157300 -- batch_size: 13 -- gen_rate: 40.9 kHz -- x_realtime: 1.9  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.00010246700710720486 \u001b[0m(-2.0398033989800317e-06)\n",
      "     | > avg_loss:\u001b[92m 9.001180330912272 \u001b[0m(-0.024644851684570312)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.vocoder.configs import WavernnConfig\n",
    "from TTS.vocoder.datasets.preprocess import load_wav_data\n",
    "from TTS.vocoder.models.wavernn import Wavernn\n",
    "\n",
    "output_path = 'results/ft_wavernn_ljspeech'\n",
    "config = WavernnConfig(\n",
    "    batch_size=128,\n",
    "    eval_batch_size=32,\n",
    "    num_loader_workers=8,\n",
    "    num_eval_loader_workers=4,\n",
    "    run_eval=True,\n",
    "    test_delay_epochs=-1,\n",
    "    epochs=10,\n",
    "    seq_len=1280,\n",
    "    pad_short=2000,\n",
    "    use_noise_augment=True,\n",
    "    eval_split_size=10,\n",
    "    print_step=25,\n",
    "    print_eval=True,\n",
    "    mixed_precision=True,\n",
    "    lr=1e-4,\n",
    "    grad_clip=4,\n",
    "    data_path='LJSpeech-1.1/wavs/',\n",
    "    output_path=output_path,\n",
    ")\n",
    "\n",
    "# init audio processor\n",
    "ap = AudioProcessor(**config.audio.to_dict())\n",
    "\n",
    "# load training samples\n",
    "eval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n",
    "\n",
    "# init model\n",
    "model = Wavernn(config)\n",
    "\n",
    "# init the trainer and ğŸš€\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(),\n",
    "    config,\n",
    "    output_path,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    "    training_assets={\"audio_processor\": ap},\n",
    ")\n",
    "trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a58ea60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:None\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:45\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /mnt)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /mnt)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      " > Training Environment:\n",
      " | > Backend: Torch\n",
      " | > Mixed precision: True\n",
      " | > Precision: fp16\n",
      " | > Current device: 0\n",
      " | > Num. of GPUs: 1\n",
      " | > Num. of CPUs: 24\n",
      " | > Num. of Torch Threads: 12\n",
      " | > Torch seed: 54321\n",
      " | > Torch CUDNN: True\n",
      " | > Torch CUDNN deterministic: False\n",
      " | > Torch CUDNN benchmark: False\n",
      " | > Torch TF32 MatMul: False\n",
      " > Start Tensorboard: tensorboard --logdir=results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "/mnt/Storage1/grozavu/conversion/.venv/lib/python3.11/site-packages/trainer/trainer.py:552: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "\n",
      " > Model has 15810401 parameters\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:10) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-25 14:27:10 -- STEP: 0/1 -- GLOBAL_STEP: 0\u001b[0m\n",
      "     | > loss: 1.5764509439468384  (1.5764509439468384)\n",
      "     | > amp_scaler: 32768.0  (32768.0)\n",
      "     | > grad_norm: 0  (0)\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.2873  (0.2873239517211914)\n",
      "     | > loader_time: 0.4084  (0.4084177017211914)\n",
      "\n",
      "/mnt/Storage1/grozavu/conversion/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time: 0.4084177017211914 \u001b[0m(+0)\n",
      "     | > avg_step_time: 0.2873239517211914 \u001b[0m(+0)\n",
      "     | > avg_loss: 1.5764509439468384 \u001b[0m(+0)\n",
      "     | > avg_amp_scaler: 32768.0 \u001b[0m(+0)\n",
      "     | > avg_grad_norm: 0 \u001b[0m(+0)\n",
      "\n",
      " > BEST MODEL : results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000/best_model_1.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 1/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:11) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.23140549659729004 \u001b[0m(-0.17701220512390137)\n",
      "     | > avg_step_time:\u001b[92m 0.27515149116516113 \u001b[0m(-0.012172460556030273)\n",
      "     | > avg_loss:\u001b[92m 1.4481086730957031 \u001b[0m(-0.12834227085113525)\n",
      "     | > avg_amp_scaler: 32768.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(28.6512, device='cuda:0') \u001b[0m(+tensor(28.6512, device='cuda:0'))\n",
      "\n",
      " > BEST MODEL : results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000/best_model_2.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 2/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:12) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.23157453536987305 \u001b[0m(+0.0001690387725830078)\n",
      "     | > avg_step_time:\u001b[92m 0.27196621894836426 \u001b[0m(-0.003185272216796875)\n",
      "     | > avg_loss:\u001b[91m 9.139028549194336 \u001b[0m(+7.690919876098633)\n",
      "     | > avg_amp_scaler:\u001b[92m 16384.0 \u001b[0m(-16384.0)\n",
      "     | > avg_grad_norm:\u001b[92m 0 \u001b[0m(tensor(-28.6512, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 3/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:12) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.22775053977966309 \u001b[0m(-0.003823995590209961)\n",
      "     | > avg_step_time:\u001b[91m 0.2722198963165283 \u001b[0m(+0.0002536773681640625)\n",
      "     | > avg_loss:\u001b[92m 8.895352363586426 \u001b[0m(-0.24367618560791016)\n",
      "     | > avg_amp_scaler:\u001b[92m 8192.0 \u001b[0m(-8192.0)\n",
      "     | > avg_grad_norm: 0 \u001b[0m(+0)\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 4/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:13) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.22120928764343262 \u001b[0m(-0.006541252136230469)\n",
      "     | > avg_step_time:\u001b[91m 0.27338671684265137 \u001b[0m(+0.0011668205261230469)\n",
      "     | > avg_loss:\u001b[92m 8.186199188232422 \u001b[0m(-0.7091531753540039)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(109.6710, device='cuda:0') \u001b[0m(+tensor(109.6710, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 5/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:14) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.2242271900177002 \u001b[0m(+0.003017902374267578)\n",
      "     | > avg_step_time:\u001b[92m 0.273327112197876 \u001b[0m(-5.9604644775390625e-05)\n",
      "     | > avg_loss:\u001b[92m 4.624033451080322 \u001b[0m(-3.5621657371520996)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(54.6513, device='cuda:0') \u001b[0m(tensor(-55.0196, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 6/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:14) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.23151373863220215 \u001b[0m(+0.007286548614501953)\n",
      "     | > avg_step_time:\u001b[91m 0.27343058586120605 \u001b[0m(+0.00010347366333007812)\n",
      "     | > avg_loss:\u001b[92m 3.067861557006836 \u001b[0m(-1.5561718940734863)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(42.7273, device='cuda:0') \u001b[0m(tensor(-11.9240, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 7/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:15) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.22770071029663086 \u001b[0m(-0.003813028335571289)\n",
      "     | > avg_step_time:\u001b[92m 0.27338099479675293 \u001b[0m(-4.9591064453125e-05)\n",
      "     | > avg_loss:\u001b[92m 2.239955186843872 \u001b[0m(-0.8279063701629639)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(50.2087, device='cuda:0') \u001b[0m(+tensor(7.4814, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 8/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:16) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.228867769241333 \u001b[0m(+0.0011670589447021484)\n",
      "     | > avg_step_time:\u001b[91m 0.2743229866027832 \u001b[0m(+0.0009419918060302734)\n",
      "     | > avg_loss:\u001b[92m 1.0022037029266357 \u001b[0m(-1.2377514839172363)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(9.5950, device='cuda:0') \u001b[0m(tensor(-40.6137, device='cuda:0'))\n",
      "\n",
      " > BEST MODEL : results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000/best_model_9.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 9/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:17) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.23253440856933594 \u001b[0m(+0.0036666393280029297)\n",
      "     | > avg_step_time:\u001b[92m 0.2735621929168701 \u001b[0m(-0.0007607936859130859)\n",
      "     | > avg_loss:\u001b[92m 0.9944039583206177 \u001b[0m(-0.007799744606018066)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(14.6755, device='cuda:0') \u001b[0m(+tensor(5.0805, device='cuda:0'))\n",
      "\n",
      " > BEST MODEL : results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000/best_model_10.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 10/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:18) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.23115062713623047 \u001b[0m(-0.0013837814331054688)\n",
      "     | > avg_step_time:\u001b[91m 0.27370548248291016 \u001b[0m(+0.00014328956604003906)\n",
      "     | > avg_loss:\u001b[92m 0.9717552065849304 \u001b[0m(-0.022648751735687256)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(11.3806, device='cuda:0') \u001b[0m(tensor(-3.2949, device='cuda:0'))\n",
      "\n",
      " > BEST MODEL : results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000/best_model_11.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 11/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:19) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.22644972801208496 \u001b[0m(-0.004700899124145508)\n",
      "     | > avg_step_time:\u001b[91m 0.2737703323364258 \u001b[0m(+6.4849853515625e-05)\n",
      "     | > avg_loss:\u001b[91m 1.2605009078979492 \u001b[0m(+0.2887457013130188)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(51.5194, device='cuda:0') \u001b[0m(+tensor(40.1388, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 12/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:19) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.23446083068847656 \u001b[0m(+0.008011102676391602)\n",
      "     | > avg_step_time:\u001b[92m 0.27350544929504395 \u001b[0m(-0.00026488304138183594)\n",
      "     | > avg_loss:\u001b[92m 1.0067073106765747 \u001b[0m(-0.2537935972213745)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(45.0487, device='cuda:0') \u001b[0m(tensor(-6.4707, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 13/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:20) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.2267158031463623 \u001b[0m(-0.007745027542114258)\n",
      "     | > avg_step_time:\u001b[91m 0.2735316753387451 \u001b[0m(+2.6226043701171875e-05)\n",
      "     | > avg_loss:\u001b[92m 0.8010882139205933 \u001b[0m(-0.20561909675598145)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(9.6045, device='cuda:0') \u001b[0m(tensor(-35.4442, device='cuda:0'))\n",
      "\n",
      " > BEST MODEL : results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000/best_model_14.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 14/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:21) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.23075461387634277 \u001b[0m(+0.004038810729980469)\n",
      "     | > avg_step_time:\u001b[91m 0.27353382110595703 \u001b[0m(+2.1457672119140625e-06)\n",
      "     | > avg_loss:\u001b[91m 1.1473393440246582 \u001b[0m(+0.34625113010406494)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(55.7913, device='cuda:0') \u001b[0m(+tensor(46.1868, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 15/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:21) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.23330903053283691 \u001b[0m(+0.0025544166564941406)\n",
      "     | > avg_step_time:\u001b[91m 0.2735919952392578 \u001b[0m(+5.817413330078125e-05)\n",
      "     | > avg_loss:\u001b[92m 0.7177873253822327 \u001b[0m(-0.42955201864242554)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(13.5976, device='cuda:0') \u001b[0m(tensor(-42.1937, device='cuda:0'))\n",
      "\n",
      " > BEST MODEL : results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000/best_model_16.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 16/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:22) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.2252042293548584 \u001b[0m(-0.008104801177978516)\n",
      "     | > avg_step_time:\u001b[91m 0.27360057830810547 \u001b[0m(+8.58306884765625e-06)\n",
      "     | > avg_loss:\u001b[92m 0.637911856174469 \u001b[0m(-0.07987546920776367)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(12.3079, device='cuda:0') \u001b[0m(tensor(-1.2897, device='cuda:0'))\n",
      "\n",
      " > BEST MODEL : results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000/best_model_17.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 17/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:23) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.22791147232055664 \u001b[0m(+0.002707242965698242)\n",
      "     | > avg_step_time:\u001b[91m 0.2736499309539795 \u001b[0m(+4.935264587402344e-05)\n",
      "     | > avg_loss:\u001b[91m 1.425659418106079 \u001b[0m(+0.7877475619316101)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(61.3373, device='cuda:0') \u001b[0m(+tensor(49.0293, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 18/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:24) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.23641252517700195 \u001b[0m(+0.008501052856445312)\n",
      "     | > avg_step_time:\u001b[91m 0.2740166187286377 \u001b[0m(+0.0003666877746582031)\n",
      "     | > avg_loss:\u001b[92m 1.03244948387146 \u001b[0m(-0.39320993423461914)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(40.0913, device='cuda:0') \u001b[0m(tensor(-21.2460, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 19/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:25) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.228377103805542 \u001b[0m(-0.008035421371459961)\n",
      "     | > avg_step_time:\u001b[91m 0.27430248260498047 \u001b[0m(+0.00028586387634277344)\n",
      "     | > avg_loss:\u001b[91m 1.3911168575286865 \u001b[0m(+0.35866737365722656)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(62.6040, device='cuda:0') \u001b[0m(+tensor(22.5127, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 20/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:25) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-25 14:27:26 -- STEP: 0/1 -- GLOBAL_STEP: 20\u001b[0m\n",
      "     | > loss: 1.4441726207733154  (1.4441726207733154)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(67.1446, device='cuda:0')  (tensor(67.1446, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.2737  (0.2736988067626953)\n",
      "     | > loader_time: 0.2334  (0.2333996295928955)\n",
      "\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.2333996295928955 \u001b[0m(+0.005022525787353516)\n",
      "     | > avg_step_time:\u001b[92m 0.2736988067626953 \u001b[0m(-0.0006036758422851562)\n",
      "     | > avg_loss:\u001b[91m 1.4441726207733154 \u001b[0m(+0.053055763244628906)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(67.1446, device='cuda:0') \u001b[0m(+tensor(4.5405, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 21/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:26) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.22730422019958496 \u001b[0m(-0.006095409393310547)\n",
      "     | > avg_step_time:\u001b[91m 0.2737157344818115 \u001b[0m(+1.6927719116210938e-05)\n",
      "     | > avg_loss:\u001b[92m 0.5842975378036499 \u001b[0m(-0.8598750829696655)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(6.9073, device='cuda:0') \u001b[0m(tensor(-60.2373, device='cuda:0'))\n",
      "\n",
      " > BEST MODEL : results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000/best_model_22.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 22/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:27) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.22223734855651855 \u001b[0m(-0.005066871643066406)\n",
      "     | > avg_step_time:\u001b[91m 0.27388834953308105 \u001b[0m(+0.00017261505126953125)\n",
      "     | > avg_loss:\u001b[91m 0.8543969392776489 \u001b[0m(+0.270099401473999)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(51.9347, device='cuda:0') \u001b[0m(+tensor(45.0274, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 23/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:28) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.2223193645477295 \u001b[0m(+8.20159912109375e-05)\n",
      "     | > avg_step_time:\u001b[92m 0.2736079692840576 \u001b[0m(-0.0002803802490234375)\n",
      "     | > avg_loss:\u001b[92m 0.8068398237228394 \u001b[0m(-0.04755711555480957)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(18.6742, device='cuda:0') \u001b[0m(tensor(-33.2605, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 24/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:28) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.232407808303833 \u001b[0m(+0.010088443756103516)\n",
      "     | > avg_step_time:\u001b[92m 0.2735593318939209 \u001b[0m(-4.863739013671875e-05)\n",
      "     | > avg_loss:\u001b[92m 0.7408385872840881 \u001b[0m(-0.06600123643875122)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(26.6272, device='cuda:0') \u001b[0m(+tensor(7.9530, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 25/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:29) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.23030424118041992 \u001b[0m(-0.002103567123413086)\n",
      "     | > avg_step_time:\u001b[91m 0.2738461494445801 \u001b[0m(+0.0002868175506591797)\n",
      "     | > avg_loss:\u001b[91m 1.275298833847046 \u001b[0m(+0.5344602465629578)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(66.5459, device='cuda:0') \u001b[0m(+tensor(39.9187, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 26/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:30) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.2224729061126709 \u001b[0m(-0.007831335067749023)\n",
      "     | > avg_step_time:\u001b[92m 0.2735147476196289 \u001b[0m(-0.0003314018249511719)\n",
      "     | > avg_loss:\u001b[92m 1.0682555437088013 \u001b[0m(-0.20704329013824463)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(54.9553, device='cuda:0') \u001b[0m(tensor(-11.5905, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 27/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:30) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.21848273277282715 \u001b[0m(-0.00399017333984375)\n",
      "     | > avg_step_time:\u001b[91m 0.27359724044799805 \u001b[0m(+8.249282836914062e-05)\n",
      "     | > avg_loss:\u001b[92m 0.9899671077728271 \u001b[0m(-0.07828843593597412)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(50.4060, device='cuda:0') \u001b[0m(tensor(-4.5494, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 28/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:31) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.230360746383667 \u001b[0m(+0.011878013610839844)\n",
      "     | > avg_step_time:\u001b[91m 0.2739288806915283 \u001b[0m(+0.00033164024353027344)\n",
      "     | > avg_loss:\u001b[91m 1.0510667562484741 \u001b[0m(+0.06109964847564697)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(55.8883, device='cuda:0') \u001b[0m(+tensor(5.4824, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 29/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:32) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.22934269905090332 \u001b[0m(-0.0010180473327636719)\n",
      "     | > avg_step_time:\u001b[92m 0.27371668815612793 \u001b[0m(-0.00021219253540039062)\n",
      "     | > avg_loss:\u001b[92m 0.833764374256134 \u001b[0m(-0.2173023819923401)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(33.5992, device='cuda:0') \u001b[0m(tensor(-22.2891, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 30/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:32) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.231400728225708 \u001b[0m(+0.0020580291748046875)\n",
      "     | > avg_step_time:\u001b[91m 0.27376389503479004 \u001b[0m(+4.7206878662109375e-05)\n",
      "     | > avg_loss:\u001b[92m 0.5934682488441467 \u001b[0m(-0.2402961254119873)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(30.9240, device='cuda:0') \u001b[0m(tensor(-2.6752, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 31/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:33) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.22908282279968262 \u001b[0m(-0.0023179054260253906)\n",
      "     | > avg_step_time:\u001b[92m 0.27373743057250977 \u001b[0m(-2.6464462280273438e-05)\n",
      "     | > avg_loss:\u001b[91m 0.9144708514213562 \u001b[0m(+0.3210026025772095)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(58.6133, device='cuda:0') \u001b[0m(+tensor(27.6893, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 32/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:34) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.23074102401733398 \u001b[0m(+0.0016582012176513672)\n",
      "     | > avg_step_time:\u001b[91m 0.2743873596191406 \u001b[0m(+0.0006499290466308594)\n",
      "     | > avg_loss:\u001b[92m 0.7359333634376526 \u001b[0m(-0.1785374879837036)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(38.8260, device='cuda:0') \u001b[0m(tensor(-19.7873, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 33/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:34) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.22550582885742188 \u001b[0m(-0.005235195159912109)\n",
      "     | > avg_step_time:\u001b[92m 0.2735903263092041 \u001b[0m(-0.0007970333099365234)\n",
      "     | > avg_loss:\u001b[91m 1.0080432891845703 \u001b[0m(+0.2721099257469177)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(45.4176, device='cuda:0') \u001b[0m(+tensor(6.5916, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 34/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:35) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.2294328212738037 \u001b[0m(+0.003926992416381836)\n",
      "     | > avg_step_time:\u001b[91m 0.2736196517944336 \u001b[0m(+2.9325485229492188e-05)\n",
      "     | > avg_loss:\u001b[91m 1.0093470811843872 \u001b[0m(+0.0013037919998168945)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(46.1913, device='cuda:0') \u001b[0m(+tensor(0.7737, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 35/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:36) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.2377605438232422 \u001b[0m(+0.008327722549438477)\n",
      "     | > avg_step_time:\u001b[92m 0.27352476119995117 \u001b[0m(-9.489059448242188e-05)\n",
      "     | > avg_loss:\u001b[92m 0.7466821074485779 \u001b[0m(-0.2626649737358093)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(28.2335, device='cuda:0') \u001b[0m(tensor(-17.9579, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 36/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:36) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.22873139381408691 \u001b[0m(-0.009029150009155273)\n",
      "     | > avg_step_time:\u001b[91m 0.27367377281188965 \u001b[0m(+0.00014901161193847656)\n",
      "     | > avg_loss:\u001b[92m 0.5047101974487305 \u001b[0m(-0.2419719099998474)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(21.8348, device='cuda:0') \u001b[0m(tensor(-6.3987, device='cuda:0'))\n",
      "\n",
      " > BEST MODEL : results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000/best_model_37.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 37/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:37) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.23371648788452148 \u001b[0m(+0.00498509407043457)\n",
      "     | > avg_step_time:\u001b[91m 0.273822546005249 \u001b[0m(+0.000148773193359375)\n",
      "     | > avg_loss:\u001b[91m 1.0355722904205322 \u001b[0m(+0.5308620929718018)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(59.0220, device='cuda:0') \u001b[0m(+tensor(37.1873, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 38/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:38) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.23030996322631836 \u001b[0m(-0.003406524658203125)\n",
      "     | > avg_step_time:\u001b[92m 0.2735459804534912 \u001b[0m(-0.0002765655517578125)\n",
      "     | > avg_loss:\u001b[92m 0.829699695110321 \u001b[0m(-0.20587259531021118)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(40.1573, device='cuda:0') \u001b[0m(tensor(-18.8647, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 39/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:39) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.23694920539855957 \u001b[0m(+0.006639242172241211)\n",
      "     | > avg_step_time:\u001b[91m 0.2735898494720459 \u001b[0m(+4.38690185546875e-05)\n",
      "     | > avg_loss:\u001b[91m 0.8498542308807373 \u001b[0m(+0.02015453577041626)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(41.4174, device='cuda:0') \u001b[0m(+tensor(1.2601, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 40/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:39) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-25 14:27:40 -- STEP: 0/1 -- GLOBAL_STEP: 40\u001b[0m\n",
      "     | > loss: 0.7969511151313782  (0.7969511151313782)\n",
      "     | > amp_scaler: 8192.0  (8192.0)\n",
      "     | > grad_norm: tensor(37.5661, device='cuda:0')  (tensor(37.5661, device='cuda:0'))\n",
      "     | > current_lr: 0.0001 \n",
      "     | > step_time: 0.2738  (0.2738063335418701)\n",
      "     | > loader_time: 0.2353  (0.235304594039917)\n",
      "\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.235304594039917 \u001b[0m(-0.0016446113586425781)\n",
      "     | > avg_step_time:\u001b[91m 0.2738063335418701 \u001b[0m(+0.00021648406982421875)\n",
      "     | > avg_loss:\u001b[92m 0.7969511151313782 \u001b[0m(-0.05290311574935913)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(37.5661, device='cuda:0') \u001b[0m(tensor(-3.8513, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 41/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:40) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.2293686866760254 \u001b[0m(-0.0059359073638916016)\n",
      "     | > avg_step_time:\u001b[92m 0.27379488945007324 \u001b[0m(-1.1444091796875e-05)\n",
      "     | > avg_loss:\u001b[92m 0.6452965140342712 \u001b[0m(-0.15165460109710693)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(30.7821, device='cuda:0') \u001b[0m(tensor(-6.7840, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 42/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:41) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.23484277725219727 \u001b[0m(+0.005474090576171875)\n",
      "     | > avg_step_time:\u001b[91m 0.27381324768066406 \u001b[0m(+1.8358230590820312e-05)\n",
      "     | > avg_loss:\u001b[92m 0.4769330620765686 \u001b[0m(-0.16836345195770264)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(23.2221, device='cuda:0') \u001b[0m(tensor(-7.5600, device='cuda:0'))\n",
      "\n",
      " > BEST MODEL : results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000/best_model_43.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 43/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:42) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.2302711009979248 \u001b[0m(-0.004571676254272461)\n",
      "     | > avg_step_time:\u001b[91m 0.2738494873046875 \u001b[0m(+3.62396240234375e-05)\n",
      "     | > avg_loss:\u001b[91m 1.0111985206604004 \u001b[0m(+0.5342654585838318)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(55.7712, device='cuda:0') \u001b[0m(+tensor(32.5490, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 44/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:42) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.23056578636169434 \u001b[0m(+0.00029468536376953125)\n",
      "     | > avg_step_time:\u001b[92m 0.2735772132873535 \u001b[0m(-0.0002722740173339844)\n",
      "     | > avg_loss:\u001b[92m 0.7371464371681213 \u001b[0m(-0.27405208349227905)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(46.0827, device='cuda:0') \u001b[0m(tensor(-9.6885, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 45/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:43) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.21899795532226562 \u001b[0m(-0.011567831039428711)\n",
      "     | > avg_step_time:\u001b[91m 0.27377986907958984 \u001b[0m(+0.00020265579223632812)\n",
      "     | > avg_loss:\u001b[92m 0.5939995050430298 \u001b[0m(-0.14314693212509155)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(28.1697, device='cuda:0') \u001b[0m(tensor(-17.9130, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 46/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:44) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.23302268981933594 \u001b[0m(+0.014024734497070312)\n",
      "     | > avg_step_time:\u001b[92m 0.2737710475921631 \u001b[0m(-8.821487426757812e-06)\n",
      "     | > avg_loss:\u001b[92m 0.5738661289215088 \u001b[0m(-0.020133376121520996)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(23.4930, device='cuda:0') \u001b[0m(tensor(-4.6767, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 47/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:44) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.23118972778320312 \u001b[0m(-0.0018329620361328125)\n",
      "     | > avg_step_time:\u001b[92m 0.2736637592315674 \u001b[0m(-0.00010728836059570312)\n",
      "     | > avg_loss:\u001b[91m 0.6537304520606995 \u001b[0m(+0.07986432313919067)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(42.7194, device='cuda:0') \u001b[0m(+tensor(19.2264, device='cuda:0'))\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 48/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:45) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.2241203784942627 \u001b[0m(-0.00706934928894043)\n",
      "     | > avg_step_time:\u001b[92m 0.2734694480895996 \u001b[0m(-0.00019431114196777344)\n",
      "     | > avg_loss:\u001b[92m 0.43008989095687866 \u001b[0m(-0.2236405611038208)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[92m tensor(30.5848, device='cuda:0') \u001b[0m(tensor(-12.1346, device='cuda:0'))\n",
      "\n",
      " > BEST MODEL : results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000/best_model_49.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 49/50\u001b[0m\n",
      " --> results/ft_wavegrad_custom_ljspeech/run-May-25-2025_02+27PM-0000000\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-25 14:27:46) \u001b[0m\n",
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.22088265419006348 \u001b[0m(-0.0032377243041992188)\n",
      "     | > avg_step_time:\u001b[91m 0.2743418216705322 \u001b[0m(+0.0008723735809326172)\n",
      "     | > avg_loss:\u001b[91m 0.8571811318397522 \u001b[0m(+0.42709124088287354)\n",
      "     | > avg_amp_scaler: 8192.0 \u001b[0m(+0.0)\n",
      "     | > avg_grad_norm:\u001b[91m tensor(50.1127, device='cuda:0') \u001b[0m(+tensor(19.5279, device='cuda:0'))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CUSTOM DATASET\n",
    "\n",
    "import os\n",
    "\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.vocoder.configs import WavegradConfig\n",
    "from TTS.vocoder.datasets.preprocess import load_wav_data\n",
    "from TTS.vocoder.models.wavegrad import Wavegrad\n",
    "\n",
    "output_path = 'results/ft_wavegrad_custom_ljspeech'\n",
    "\n",
    "config = WavegradConfig(\n",
    "    batch_size=64,                # Aggressive; safe for 24GB VRAM\n",
    "    eval_batch_size=32,\n",
    "    num_loader_workers=8,         # Use multiple CPU threads for I/O\n",
    "    num_eval_loader_workers=4,\n",
    "    run_eval=False,\n",
    "    test_delay_epochs=100,  # No eval during training\n",
    "    epochs=50,                  # Full training cycle\n",
    "    seq_len=16384,                # Very long sequences = better audio quality\n",
    "    pad_short=3000,\n",
    "    use_noise_augment=True,\n",
    "    eval_split_size=0,          # Larger eval set for robust monitoring\n",
    "    print_step=20,\n",
    "    print_eval=True,\n",
    "    mixed_precision=True,         # Essential for best performance on 4090\n",
    "    data_path=\"Custom_LJSpeech/wavs\",\n",
    "    output_path=output_path,\n",
    ")\n",
    "\n",
    "# init audio processor\n",
    "ap = AudioProcessor(**config.audio.to_dict())\n",
    "\n",
    "# load training samples\n",
    "eval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n",
    "\n",
    "# init model\n",
    "model = Wavegrad(config)\n",
    "\n",
    "# init the trainer and ğŸš€\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(),\n",
    "    config,\n",
    "    output_path,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    "    training_assets={\"audio_processor\": ap},\n",
    ")\n",
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
